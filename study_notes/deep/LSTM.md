RNN(êµì•ˆ 8)ì€ NLPì˜ ê¸°ë³¸ ì•„ì´ë””ì–´ì˜€ì§€ë§Œ, í•œ ê°€ì§€ ì¹˜ëª…ì ì¸ ì•½ì ì´ ìžˆì—ˆìŠµë‹ˆë‹¤.

ë°”ë¡œ **ìž¥ê¸° ì˜ì¡´ì„± ë¬¸ì œ (Long-Term Dependency Problem)**ìž…ë‹ˆë‹¤.

ë¬¸ìž¥ì´ ê¸¸ì–´ì§€ë©´ (ì˜ˆ: "ë‚˜ëŠ” ì–´ì œ ì•„ì£¼ ë¨¼ ê³³ì— ìžˆëŠ” ì¹œêµ¬ë¥¼ ë§Œë‚˜ëŸ¬ ê°”ëŠ”ë°...") ë¬¸ìž¥ ë§¨ ì•žì˜ 'ë‚˜' ë˜ëŠ” 'ì¹œêµ¬'ë¼ëŠ” ì •ë³´ë¥¼ ë¬¸ìž¥ ëê¹Œì§€ 'ê¸°ì–µ'í•˜ê¸°ê°€ ë§¤ìš° ì–´ë µìŠµë‹ˆë‹¤. (ìˆ˜í•™ì ìœ¼ë¡œëŠ” 'ê¸°ìš¸ê¸° ì†Œì‹¤(Vanishing Gradient)' ë¬¸ì œë¼ê³  ë¶€ë¦…ë‹ˆë‹¤.)

ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë“±ìž¥í•œ ê²ƒì´ ë°”ë¡œ **LSTM (Long Short-Term Memory, ìž¥ë‹¨ê¸° ê¸°ì–µ ë©”ëª¨ë¦¬)**ê³¼ **GRU**ìž…ë‹ˆë‹¤.

ì´ LSTMì´ 2010ë…„ëŒ€ NLPë¥¼ ì§€ë°°í–ˆë˜ ì‚¬ì‹¤ìƒì˜ í‘œì¤€ ëª¨ë¸ìž…ë‹ˆë‹¤.


## RNNì˜ ì§„í™”: LSTM (Long Short-Term Memory)

**1. RNNì˜ í•œê³„: ìž¥ê¸° ì˜ì¡´ì„± ë¬¸ì œ (Long-Term Dependency)**

- **ë¬¸ì œ:** RNNì€ ì‹œí€€ìŠ¤(ìˆœì„œ)ê°€ ê¸¸ì–´ì§€ë©´ (ì˜ˆ: ê¸´ ë¬¸ìž¥, ê¸´ ì‹œê³„ì—´ ë°ì´í„°) ë§¨ ì²˜ìŒì˜ ì •ë³´ê°€ ë’¤ë¡œ ê°ˆìˆ˜ë¡ **ì ì  ìžŠíž™ë‹ˆë‹¤.**
    
- "ë‚˜ëŠ” ì–´ì œ ì•„ì£¼ ë¨¼ ê³³ì— ìžˆëŠ” ì¹œêµ¬ë¥¼ ë§Œë‚˜ëŸ¬ ê°”ëŠ”ë°... [... 30ë‹¨ì–´ í›„ ...] ê·¸ ì¹œêµ¬ëŠ” ( )ì„ ì¢‹ì•„í–ˆë‹¤."
    
- ì´ë•Œ ( )ë¥¼ ì˜ˆì¸¡í•˜ë ¤ë©´ ë§¨ ì•žì˜ 'ì¹œêµ¬' ì •ë³´ë¥¼ ê¸°ì–µí•´ì•¼ í•˜ëŠ”ë°, 30ë‹¨ê³„ë¥¼ ê±°ì¹˜ë©´ì„œ RNNì˜ 'ê¸°ì–µ($h_t$)' ì†ì—ì„œ ì´ ì •ë³´ê°€ í¬ë¯¸í•´ì§‘ë‹ˆë‹¤.
    

**2. LSTMì˜ ì•„ì´ë””ì–´: "ê²Œì´íŠ¸ (Gates)" ë°¸ë¸Œ ðŸšª**

- LSTMì€ ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ 'ê¸°ì–µ' $h_t$ (Hidden State) ì™¸ì— **'ìž¥ê¸° ê¸°ì–µ' $C_t$ (Cell State)**ë¼ëŠ” ë³„ë„ì˜ ê¸°ì–µ í†µë¡œë¥¼ ë§Œë“­ë‹ˆë‹¤.
    
- ê·¸ë¦¬ê³  ì´ $C_t$ë¥¼ ì œì–´í•˜ê¸° ìœ„í•´ 3ê°œì˜ 'ê²Œì´íŠ¸(ë°¸ë¸Œ)'ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
    
    1. **Forget Gate (ë§ê° ê²Œì´íŠ¸):** "ì´ì „ ê¸°ì–µ($C_{t-1}$)ì—ì„œ ì–´ë–¤ ì •ë³´ë¥¼ **ìžŠì–´ë²„ë¦´ì§€**" ê²°ì •.
        
    2. **Input Gate (ìž…ë ¥ ê²Œì´íŠ¸):** "í˜„ìž¬ ì •ë³´($x_t$)ì—ì„œ ì–´ë–¤ ê²ƒì„ **ìƒˆë¡œ ê¸°ì–µí• ì§€**" ê²°ì •.
        
    3. **Output Gate (ì¶œë ¥ ê²Œì´íŠ¸):** "ìž¥ê¸° ê¸°ì–µ($C_t$) ì¤‘ì—ì„œ ì–´ë–¤ ê²ƒì„ **ì´ë²ˆ ë‹¨ê³„ì˜ ì¶œë ¥($h_t$)ìœ¼ë¡œ ë‚´ë³´ë‚¼ì§€**" ê²°ì •.
        
- ì´ ê²Œì´íŠ¸ë“¤ ë•ë¶„ì— LSTMì€ 100ë‹¨ê³„ ì „ì˜ ì •ë³´ë¼ë„ 'ìžŠì§€ ì•Šê¸°'ë¡œ ê²°ì •í•˜ë©´ ê·¸ ì •ë³´ë¥¼ ê±°ì˜ ê·¸ëŒ€ë¡œ ë³´ì¡´í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
    

**3. PyTorch `nn.LSTM`**

- ì‚¬ìš©ë²•ì€ `nn.RNN`ê³¼ ê±°ì˜ ë™ì¼í•©ë‹ˆë‹¤!
    
- ê°€ìž¥ í° ì°¨ì´ì : RNNì€ `h_0` (ì´ˆê¸° ì€ë‹‰ ìƒíƒœ)ë§Œ í•„ìš”í–ˆì§€ë§Œ, LSTMì€ **`(h_0, c_0)`** (ì´ˆê¸° ì€ë‹‰ ìƒíƒœ, **ì´ˆê¸° ì…€ ìƒíƒœ**) ë‘ ê°œë¥¼ í•„ìš”ë¡œ í•©ë‹ˆë‹¤.

RNN(êµì•ˆ 8-2)ì—ì„œ í’€ì—ˆë˜ ë™ì¼í•œ `Many-to-One` ë¬¸ì œë¥¼ LSTMìœ¼ë¡œ í’€ì–´ë³´ê² ìŠµë‹ˆë‹¤. ì½”ë“œê°€ ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•œì§€ ë¹„êµí•´ ë³´ì„¸ìš”.

```python
import torch
import torch.nn as nn
import torch.optim as optim

# --- 0. ë°ì´í„° ì¤€ë¹„ ---
# [Batch, Sequence Length, Input_dim]
x_train = torch.tensor([
    [[8.0], [1.0], [5.0], [3.0]],  # Sample 1 (Class 0)
    [[2.0], [7.0], [4.0], [9.0]]   # Sample 2 (Class 1)
])
y_train = torch.tensor([[0.0], [1.0]]) # Sample 1 -> 0, Sample 2 -> 1

# --- í•˜ì´í¼íŒŒë¼ë¯¸í„° ---
input_size = 1   # ìž…ë ¥ 1ê°œ(ì˜ˆ: 8.0)ì˜ ì°¨ì›
hidden_size = 5  # 'ê¸°ì–µ(hidden/cell state)'ì˜ í¬ê¸°
num_layers = 1   # LSTM ì¸µì„ 1ê°œë§Œ ìŒ“ìŒ
output_size = 1  # ìµœì¢… ì¶œë ¥ (0 ë˜ëŠ” 1)

# --- 1. ëª¨ë¸ ì •ì˜ (class ì‚¬ìš©) ---
class SimpleLSTMModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.layer_dim = layer_dim
        
        # 1. LSTM ë ˆì´ì–´ ì •ì˜
        # nn.RNN ëŒ€ì‹  nn.LSTMì„ ì‚¬ìš©!
        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)
        
        # 2. ìµœì¢… ì¶œë ¥ì„ ìœ„í•œ Linear ë ˆì´ì–´
        self.fc = nn.Linear(hidden_dim, output_dim)
        
        # 3. ì¶œë ¥ì„ 0~1 ì‚¬ì´ í™•ë¥ ë¡œ
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        # 1. ì´ˆê¸° ì€ë‹‰ ìƒíƒœ(h_0)ì™€ ì…€ ìƒíƒœ(c_0)ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”
        # (layer_dim, batch_size, hidden_dim)
        h_0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)
        c_0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)
        
        # 2. LSTM ì‹¤í–‰
        # RNNê³¼ ë‹¬ë¦¬ (h_n, c_n) ë‘ ê°œì˜ ìƒíƒœë¥¼ ë°˜í™˜
        # lstm_out: ëª¨ë“  ì‹œí€€ìŠ¤ ë‹¨ê³„ì˜ ì¶œë ¥(hidden state)
        # (h_n, c_n): ë§ˆì§€ë§‰ ë‹¨ê³„ì˜ (ì€ë‹‰ ìƒíƒœ, ì…€ ìƒíƒœ)
        lstm_out, (h_n, c_n) = self.lstm(x, (h_0, c_0))
        
        # 3. Many-to-One: ìš°ë¦¬ëŠ” ë§ˆì§€ë§‰ 'ì€ë‹‰ ìƒíƒœ' h_në§Œ í•„ìš”
        # h_nì˜ shape: (num_layers, batch_size, hidden_size) -> (1, 2, 5)
        # [0]ì„ í†µí•´ (batch_size, hidden_size)ë¡œ ë³€ê²½ -> (2, 5)
        h_n_last = h_n[0] 
        
        # 4. Linear ë ˆì´ì–´ í†µê³¼
        out = self.fc(h_n_last)
        
        # 5. Sigmoid í™œì„±í™”
        out = self.sigmoid(out)
        
        return out

# ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤(ê°ì²´) ìƒì„±
model = SimpleLSTMModel(input_size, hidden_size, num_layers, output_size)

# --- 2. ì†ì‹¤ í•¨ìˆ˜(Loss) ë° ì˜µí‹°ë§ˆì´ì €(Optimizer) ì •ì˜ ---
criterion = nn.BCELoss() # ë°”ì´ë„ˆë¦¬ ë¶„ë¥˜
optimizer = optim.SGD(model.parameters(), lr=0.1)

# --- 3. í•™ìŠµ(Training) ì‹¤í–‰ ---
epochs = 500

for epoch in range(epochs + 1):
    prediction = model(x_train)
    loss = criterion(prediction, y_train)
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if epoch % 50 == 0:
        predicted_classes = (prediction >= 0.5).float()
        accuracy = (predicted_classes == y_train).float().mean()
        print(f'Epoch {epoch:4d}/{epochs} | Loss: {loss.item():.6f} | Accuracy: {accuracy.item() * 100:.2f}%')

# --- 4. í•™ìŠµ ê²°ê³¼ í™•ì¸ ---
print("\n--- í•™ìŠµ ì™„ë£Œ í›„ ---")
final_prediction = (model(x_train) >= 0.5).float()
for i in range(len(x_train)):
    print(f"Input: {x_train[i].view(-1).tolist()} | ì •ë‹µ: {y_train[i].item()} | ì˜ˆì¸¡: {final_prediction[i].item()}")
```