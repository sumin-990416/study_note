### `Dataset` & `DataLoader`

PyTorch는 이 문제를 두 부분으로 나눠서 해결합니다.

**1. `Dataset` 📦 (데이터 지도 또는 청사진)**

- `Dataset` 클래스는 "내 모든 데이터가 어디에, 어떻게 저장되어 있는지" 그 **청사진(blueprint)**을 정의합니다.
    
- 이 클래스가 할 일은 딱 두 가지입니다.
    
    1. `__len__(self)`: "전체 데이터가 총 몇 개인가요?" (예: 10,000개)
        
    2. `__getitem__(self, idx)`: "그중에서 `idx`번째 데이터 한 개만 주세요." (예: 5번째 이미지 파일과 그 정답(label)을 반환)
        
- 즉, `Dataset`은 데이터 전체를 불러오는 것이 아니라, **"어떻게 데이터 한 개를 가져올지"** 그 방법만 알려줍니다.
    

**2. `DataLoader` 🚚 (데이터 배송 트럭)**

- `DataLoader`는 `Dataset`(청사진)를 입력으로 받아, 실제 데이터 배송을 총괄하는 **배송 트럭** 역할을 합니다.
    
- `DataLoader`가 알아서 해주는 핵심 작업들은 다음과 같습니다.
    
    - **배치(Batching):** `Dataset`에서 데이터 1개씩 가져와, 우리가 정한 `batch_size` (예: 32개)만큼 묶어서 **미니배치(mini-batch)**를 만듭니다. GPU는 데이터를 한 개씩 처리하는 것보다 묶음(배치)으로 처리할 때 훨씬 효율적입니다.
        
    - **셔플(Shuffling):** 모델이 데이터 순서를 외우지 못하도록, 매 학습 주기(epoch)마다 데이터를 무작위로 섞어줍니다. (`shuffle=True`)
        
    - **병렬 처리(Parallel Loading):** GPU가 현재 배치로 열심히 학습하는 동안, CPU가 쉬지 않고 **미리 다음 배치를 준비**시킵니다. (`num_workers` 설정) 덕분에 GPU가 데이터 로딩을 기다리는 시간이 사라져 학습 속도가 매우 빨라집니다.

```python
import torch
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torch.optim as optim

# --- 1. Dataset 청사진 정의 ---
# (실제로는 이미지 파일 경로 등을 다루지만, 여기서는 간단한 텐서로 대체)
class MyCustomDataset(Dataset):
    
    # 데이터의 '청사진'을 초기화
    def __init__(self, x_data, y_data):
        self.x_data = x_data # 모든 x 데이터 (예: 10000개)
        self.y_data = y_data # 모든 y 데이터 (예: 10000개)
    
    # "Q. 총 데이터 개수가 몇 개인가요?"
    def __len__(self):
        return len(self.x_data)
    
    # "Q. idx번째 데이터 1개를 주세요."
    def __getitem__(self, idx):
        x = self.x_data[idx]
        y = self.y_data[idx]
        # (만약 이미지라면, 여기서 이미지 회전, 자르기 등 전처리 수행)
        return x, y

# --- 2. 데이터 준비 및 DataLoader 생성 ---
# 가상의 대용량 데이터 (1000개의 샘플)
all_x_data = torch.randn(1000, 10) # 1000개, 특성 10개
all_y_data = torch.randn(1000, 1)  # 1000개, 정답 1개

# 1. 청사진(Dataset) 인스턴스 생성
train_dataset = MyCustomDataset(all_x_data, all_y_data)

# 2. 배송 트럭(DataLoader) 생성
# "train_dataset 청사진을 보고, 32개씩 묶어서(batch_size), 
#  매번 순서를 섞고(shuffle), CPU 4개를 써서(num_workers) 가져와줘!"
train_loader = DataLoader(train_dataset, 
                          batch_size=32, 
                          shuffle=True, 
                          num_workers=4)

# (모델, 손실함수, 옵티마이저 정의는 동일하다고 가정)
model = nn.Linear(10, 1) # 예시 모델
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)
epochs = 10

# --- 3. 새로워진 학습(Training) 반복문 ---
for epoch in range(epochs):
    
    # train_loader가 알아서 32개(batch_size)씩 데이터를 꺼내줍니다!
    # (1000 / 32 = 약 32번 반복)
    for batch_x, batch_y in train_loader:
        
        # (데이터를 GPU로 이동시키는 코드 (예: batch_x.to(device)))
        
        # 1. (Forward) 모델 예측 (이제 32개씩 처리)
        prediction = model(batch_x)
        
        # 2. (Forward) 손실 계산
        loss = criterion(prediction, batch_y)
        
        # 3, 4, 5단계 (Backward 및 업데이트)는 동일
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    print(f"Epoch {epoch+1}/{epochs} 완료.")
```