### `Dataset` & `DataLoader`

PyTorchëŠ” ì´ ë¬¸ì œë¥¼ ë‘ ë¶€ë¶„ìœ¼ë¡œ ë‚˜ëˆ ì„œ í•´ê²°í•©ë‹ˆë‹¤.

**1. `Dataset` ğŸ“¦ (ë°ì´í„° ì§€ë„ ë˜ëŠ” ì²­ì‚¬ì§„)**

- `Dataset` í´ë˜ìŠ¤ëŠ” "ë‚´ ëª¨ë“  ë°ì´í„°ê°€ ì–´ë””ì—, ì–´ë–»ê²Œ ì €ì¥ë˜ì–´ ìˆëŠ”ì§€" ê·¸ **ì²­ì‚¬ì§„(blueprint)**ì„ ì •ì˜í•©ë‹ˆë‹¤.
    
- ì´ í´ë˜ìŠ¤ê°€ í•  ì¼ì€ ë”± ë‘ ê°€ì§€ì…ë‹ˆë‹¤.
    
    1. `__len__(self)`: "ì „ì²´ ë°ì´í„°ê°€ ì´ ëª‡ ê°œì¸ê°€ìš”?" (ì˜ˆ: 10,000ê°œ)
        
    2. `__getitem__(self, idx)`: "ê·¸ì¤‘ì—ì„œ `idx`ë²ˆì§¸ ë°ì´í„° í•œ ê°œë§Œ ì£¼ì„¸ìš”." (ì˜ˆ: 5ë²ˆì§¸ ì´ë¯¸ì§€ íŒŒì¼ê³¼ ê·¸ ì •ë‹µ(label)ì„ ë°˜í™˜)
        
- ì¦‰, `Dataset`ì€ ë°ì´í„° ì „ì²´ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, **"ì–´ë–»ê²Œ ë°ì´í„° í•œ ê°œë¥¼ ê°€ì ¸ì˜¬ì§€"** ê·¸ ë°©ë²•ë§Œ ì•Œë ¤ì¤ë‹ˆë‹¤.
    

**2. `DataLoader` ğŸšš (ë°ì´í„° ë°°ì†¡ íŠ¸ëŸ­)**

- `DataLoader`ëŠ” `Dataset`(ì²­ì‚¬ì§„)ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„, ì‹¤ì œ ë°ì´í„° ë°°ì†¡ì„ ì´ê´„í•˜ëŠ” **ë°°ì†¡ íŠ¸ëŸ­** ì—­í• ì„ í•©ë‹ˆë‹¤.
    
- `DataLoader`ê°€ ì•Œì•„ì„œ í•´ì£¼ëŠ” í•µì‹¬ ì‘ì—…ë“¤ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
    
    - **ë°°ì¹˜(Batching):** `Dataset`ì—ì„œ ë°ì´í„° 1ê°œì”© ê°€ì ¸ì™€, ìš°ë¦¬ê°€ ì •í•œ `batch_size` (ì˜ˆ: 32ê°œ)ë§Œí¼ ë¬¶ì–´ì„œ **ë¯¸ë‹ˆë°°ì¹˜(mini-batch)**ë¥¼ ë§Œë“­ë‹ˆë‹¤. GPUëŠ” ë°ì´í„°ë¥¼ í•œ ê°œì”© ì²˜ë¦¬í•˜ëŠ” ê²ƒë³´ë‹¤ ë¬¶ìŒ(ë°°ì¹˜)ìœ¼ë¡œ ì²˜ë¦¬í•  ë•Œ í›¨ì”¬ íš¨ìœ¨ì ì…ë‹ˆë‹¤.
        
    - **ì…”í”Œ(Shuffling):** ëª¨ë¸ì´ ë°ì´í„° ìˆœì„œë¥¼ ì™¸ìš°ì§€ ëª»í•˜ë„ë¡, ë§¤ í•™ìŠµ ì£¼ê¸°(epoch)ë§ˆë‹¤ ë°ì´í„°ë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ì–´ì¤ë‹ˆë‹¤. (`shuffle=True`)
        
    - **ë³‘ë ¬ ì²˜ë¦¬(Parallel Loading):** GPUê°€ í˜„ì¬ ë°°ì¹˜ë¡œ ì—´ì‹¬íˆ í•™ìŠµí•˜ëŠ” ë™ì•ˆ, CPUê°€ ì‰¬ì§€ ì•Šê³  **ë¯¸ë¦¬ ë‹¤ìŒ ë°°ì¹˜ë¥¼ ì¤€ë¹„**ì‹œí‚µë‹ˆë‹¤. (`num_workers` ì„¤ì •) ë•ë¶„ì— GPUê°€ ë°ì´í„° ë¡œë”©ì„ ê¸°ë‹¤ë¦¬ëŠ” ì‹œê°„ì´ ì‚¬ë¼ì ¸ í•™ìŠµ ì†ë„ê°€ ë§¤ìš° ë¹¨ë¼ì§‘ë‹ˆë‹¤.

```python
import torch
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torch.optim as optim

# --- 1. Dataset ì²­ì‚¬ì§„ ì •ì˜ ---
# (ì‹¤ì œë¡œëŠ” ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ë“±ì„ ë‹¤ë£¨ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ í…ì„œë¡œ ëŒ€ì²´)
class MyCustomDataset(Dataset):
    
    # ë°ì´í„°ì˜ 'ì²­ì‚¬ì§„'ì„ ì´ˆê¸°í™”
    def __init__(self, x_data, y_data):
        self.x_data = x_data # ëª¨ë“  x ë°ì´í„° (ì˜ˆ: 10000ê°œ)
        self.y_data = y_data # ëª¨ë“  y ë°ì´í„° (ì˜ˆ: 10000ê°œ)
    
    # "Q. ì´ ë°ì´í„° ê°œìˆ˜ê°€ ëª‡ ê°œì¸ê°€ìš”?"
    def __len__(self):
        return len(self.x_data)
    
    # "Q. idxë²ˆì§¸ ë°ì´í„° 1ê°œë¥¼ ì£¼ì„¸ìš”."
    def __getitem__(self, idx):
        x = self.x_data[idx]
        y = self.y_data[idx]
        # (ë§Œì•½ ì´ë¯¸ì§€ë¼ë©´, ì—¬ê¸°ì„œ ì´ë¯¸ì§€ íšŒì „, ìë¥´ê¸° ë“± ì „ì²˜ë¦¬ ìˆ˜í–‰)
        return x, y

# --- 2. ë°ì´í„° ì¤€ë¹„ ë° DataLoader ìƒì„± ---
# ê°€ìƒì˜ ëŒ€ìš©ëŸ‰ ë°ì´í„° (1000ê°œì˜ ìƒ˜í”Œ)
all_x_data = torch.randn(1000, 10) # 1000ê°œ, íŠ¹ì„± 10ê°œ
all_y_data = torch.randn(1000, 1)  # 1000ê°œ, ì •ë‹µ 1ê°œ

# 1. ì²­ì‚¬ì§„(Dataset) ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
train_dataset = MyCustomDataset(all_x_data, all_y_data)

# 2. ë°°ì†¡ íŠ¸ëŸ­(DataLoader) ìƒì„±
# "train_dataset ì²­ì‚¬ì§„ì„ ë³´ê³ , 32ê°œì”© ë¬¶ì–´ì„œ(batch_size), 
#  ë§¤ë²ˆ ìˆœì„œë¥¼ ì„ê³ (shuffle), CPU 4ê°œë¥¼ ì¨ì„œ(num_workers) ê°€ì ¸ì™€ì¤˜!"
train_loader = DataLoader(train_dataset, 
                          batch_size=32, 
                          shuffle=True, 
                          num_workers=4)

# (ëª¨ë¸, ì†ì‹¤í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì € ì •ì˜ëŠ” ë™ì¼í•˜ë‹¤ê³  ê°€ì •)
model = nn.Linear(10, 1) # ì˜ˆì‹œ ëª¨ë¸
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)
epochs = 10

# --- 3. ìƒˆë¡œì›Œì§„ í•™ìŠµ(Training) ë°˜ë³µë¬¸ ---
for epoch in range(epochs):
    
    # train_loaderê°€ ì•Œì•„ì„œ 32ê°œ(batch_size)ì”© ë°ì´í„°ë¥¼ êº¼ë‚´ì¤ë‹ˆë‹¤!
    # (1000 / 32 = ì•½ 32ë²ˆ ë°˜ë³µ)
    for batch_x, batch_y in train_loader:
        
        # (ë°ì´í„°ë¥¼ GPUë¡œ ì´ë™ì‹œí‚¤ëŠ” ì½”ë“œ (ì˜ˆ: batch_x.to(device)))
        
        # 1. (Forward) ëª¨ë¸ ì˜ˆì¸¡ (ì´ì œ 32ê°œì”© ì²˜ë¦¬)
        prediction = model(batch_x)
        
        # 2. (Forward) ì†ì‹¤ ê³„ì‚°
        loss = criterion(prediction, batch_y)
        
        # 3, 4, 5ë‹¨ê³„ (Backward ë° ì—…ë°ì´íŠ¸)ëŠ” ë™ì¼
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    print(f"Epoch {epoch+1}/{epochs} ì™„ë£Œ.")
```