**1. 개념: 왜 선형 회귀는 분류에 적합하지 않을까?**

- **선형 회귀** $H(x) = Wx + b$ 의 예측값은 **모든 실수**입니다. (예: -100, 0.5, 999)
    
- 하지만 **분류** 문제 (예: 합격/불합격, 스팸/정상)의 정답은 **0 또는 1**입니다.
    
- 선형 회귀의 예측값 $\rightarrow$ 분류 문제의 정답
    
    - `150.7` $\rightarrow$ `1 (합격)`
        
    - `-30.2` $\rightarrow$ `0 (불합격)`
        
- 선형 회귀의 예측값을 0과 1 사이의 '확률' 값으로 바꿔줄 **'활성화 함수(Activation Function)'**가 필요합니다.
    

**2. 핵심 아이디어: 시그모이드 함수 (Sigmoid Function) sigmoid**

- 로지스틱 회귀는 선형 회귀의 예측값 $z = Wx + b$ 를 **시그모이드 함수** $\sigma(z)$ 에 통과시킵니다.
    
- 시그모이드 함수는 어떤 실수 $z$ 가 들어오든, 그 결과를 항상 **0과 1 사이의 값 (확률)**로 변환해 줍니다.
    
    - $z$ 가 매우 크면 $\rightarrow$ 1에 가까워집니다. (예: 합격 확률 99%)
        
    - $z$ 가 매우 작으면 $\rightarrow$ 0에 가까워집니다. (예: 합격 확률 1%)
        
    - $z$ 가 0이면 $\rightarrow$ 0.5가 됩니다.

![[Pasted image 20251021105826.png]]
- 가설 (Hypothesis) 수식:
    
    $$H(x) = \sigma(Wx + b)$$
    
- 시그모이드 함수 $\sigma(z)$ 수식:
    
    $$\sigma(z) = \frac{1}{1 + e^{-z}}$$
    

**3. 새로운 손실 함수: 바이너리 크로스 엔트로피 (BCE - Binary Cross Entrophy)**

- '값'의 차이를 재던 MSE 손실 함수는 **'확률'**을 다루는 분류 문제에 적합하지 않습니다. (특히 0 또는 1로 예측이 몰릴 때 미분값이 0에 가까워져 학습이 멈추는 문제가 발생합니다.)
    
- 분류 문제에서는 **크로스 엔트로피(Cross-Entropy)**라는 손실 함수를 사용합니다.
    
- **BCE의 의미:** 모델이 예측한 확률( $H(x)$ )이 실제 정답( $y$ )과 얼마나 다른지 측정합니다.
    
    - **실제 정답 $y=1$ 일 때:**
        
        - 모델이 `0.99` (1에 가깝게) 예측 $\rightarrow$ 손실(오차)이 **낮아짐**
            
        - 모델이 `0.01` (0에 가깝게) 예측 $\rightarrow$ 손실(오차)이 **매우 높아짐**
            
    - **실제 정답 $y=0$ 일 때:**
        
        - 모델이 `0.01` (0에 가깝게) 예측 $\rightarrow$ 손실(오차)이 **낮아짐**
            
        - 모델이 `0.99` (1에 가깝게) 예측 $\rightarrow$ 손실(오차)이 **매우 높아짐**
            
- BCE 손실 함수 수식:
    
    $$\text{Loss} = -\frac{1}{N} \sum_{i=1}^{N} \left[ y_i \log(H(x_i)) + (1 - y_i) \log(1 - H(x_i)) \right]$$


```python
import torch
import torch.nn as nn
import torch.optim as optim

# --- 0. 데이터 준비 ---
# 공부 시간(x)에 따른 합격/불합격(y) 데이터
# [1시간] -> 0 (불합격), [2시간] -> 0, [3시간] -> 0
# [4시간] -> 1 (합격),  [5시간] -> 1, [6시간] -> 1
x_train = torch.tensor([[1.0], [2.0], [3.0], [4.0], [5.0], [6.0]])
y_train = torch.tensor([[0.0], [0.0], [0.0], [1.0], [1.0], [1.0]])

# --- 1. 모델 정의 ---
# 로지스틱 회귀는 선형 회귀(nn.Linear)와 시그모이드 함수(nn.Sigmoid)의 조합입니다.
# nn.Sequential을 사용해 이 두 개를 순차적으로 연결할 수 있습니다.
model = nn.Sequential(
    nn.Linear(1, 1),  # H(x) = Wx + b (여기까지가 선형 회귀)
    nn.Sigmoid()      # output = sigmoid(Wx + b) (여기까지가 로지스틱 회귀)
)

# --- 2. 손실 함수(Loss) 및 옵티마이저(Optimizer) 정의 ---
# 손실 함수로 바이너리 크로스 엔트로피(BCE)를 사용
# (참고: PyTorch의 nn.BCELoss()를 사용합니다)
criterion = nn.BCELoss()

# 옵티마이저로 SGD 사용
optimizer = optim.SGD(model.parameters(), lr=0.1) 

# --- 3. 학습(Training) 실행 ---
epochs = 1000

for epoch in range(epochs + 1):
    
    # 1. (Forward) 모델 예측 (0~1 사이의 확률값)
    prediction = model(x_train) # H(x) 계산
    
    # 2. (Forward) 손실(Loss) 계산 (BCE Loss)
    loss = criterion(prediction, y_train)
    
    # 3. (Backward 준비) 미분값 초기화
    optimizer.zero_grad()
    
    # 4. (Backward) 자동 미분 실행
    loss.backward()
    
    # 5. (Update) 파라미터 업데이트
    optimizer.step()
    
    if epoch % 100 == 0:
        # prediction >= 0.5 이면 True(1), 아니면 False(0)로 변환
        predicted_classes = (prediction >= 0.5).float() 
        # 실제 정답(y_train)과 예측된 클래스(predicted_classes)가 얼마나 일치하는지 계산
        accuracy = (predicted_classes == y_train).float().mean()
        print(f'Epoch {epoch:4d}/{epochs} | Loss: {loss.item():.6f} | Accuracy: {accuracy.item() * 100:.2f}%')

# --- 4. 학습 결과 확인 ---
print("\n--- 학습 완료 후 ---")
# 3.5시간 공부했을 때의 합격 확률 예측
new_input = torch.tensor([[3.5]])
predicted_prob = model(new_input)
print(f"x=3.5 시간일 때 합격 확률: {predicted_prob.item() * 100:.2f}%")
```


```python
import torch
import torch.nn as nn
import torch.optim as optim

# --- 0. 데이터 준비 ---
# 공부 시간(x)에 따른 합격/불합격(y) 데이터
# [1시간] -> 0 (불합격), [2시간] -> 0, [3시간] -> 0
# [4시간] -> 1 (합격),  [5시간] -> 1, [6시간] -> 1
x_train = torch.tensor([[1.0], [2.0], [3.0], [4.0], [5.0], [6.0]])
y_train = torch.tensor([[0.0], [0.0], [0.0], [1.0], [1.0], [1.0]])


# --- 1. 모델 정의 (class 사용) ---
# H(x) = Sigmoid(Wx + b)
class LogisticRegressionModel(nn.Module):
    def __init__(self, input_dim, output_dim):
        super().__init__()
        # 선형 층과 시그모이드 함수를 순차적으로 정의
        self.layers = nn.Sequential(
            nn.Linear(input_dim, output_dim), # Wx + b
            nn.Sigmoid()                      # Sigmoid(...)
        )
    
    def forward(self, x):
        # 정의된 self.layers에 x를 통과시킴
        return self.layers(x)

# 모델 인스턴스(객체) 생성
# 입력 특성은 1개(공부 시간), 출력 특성도 1개(합격 확률)
model = LogisticRegressionModel(input_dim=1, output_dim=1)


# --- 2. 손실 함수(Loss) 및 옵티마이저(Optimizer) 정의 ---
# 손실 함수로 바이너리 크로스 엔트로피(BCE)를 사용
criterion = nn.BCELoss()
# 옵티마이저로 SGD 사용
optimizer = optim.SGD(model.parameters(), lr=0.1) 


# --- 3. 학습(Training) 실행 ---
epochs = 2000 # 로지스틱 회귀는 학습이 좀 더 필요할 수 있습니다.

for epoch in range(epochs + 1):
    
    # 1. (Forward) 모델 예측 (0~1 사이의 확률값)
    prediction = model(x_train) # H(x) 계산
    
    # 2. (Forward) 손실(Loss) 계산 (BCE Loss)
    loss = criterion(prediction, y_train)
    
    # 3. (Backward 준비) 미분값 초기화
    optimizer.zero_grad()
    
    # 4. (Backward) 자동 미분 실행
    loss.backward()
    
    # 5. (Update) 파라미터 업데이트
    optimizer.step()
    
    if epoch % 200 == 0:
        # prediction >= 0.5 이면 True(1), 아니면 False(0)로 변환
        predicted_classes = (prediction >= 0.5).float() 
        # 실제 정답(y_train)과 예측된 클래스(predicted_classes)가 얼마나 일치하는지 계산
        accuracy = (predicted_classes == y_train).float().mean()
        print(f'Epoch {epoch:4d}/{epochs} | Loss: {loss.item():.6f} | Accuracy: {accuracy.item() * 100:.2f}%')

# --- 4. 학습 결과 확인 ---
print("\n--- 학습 완료 후 ---")
# 3.5시간 공부했을 때의 합격 확률 예측
new_input = torch.tensor([[3.5]])
predicted_prob = model(new_input)
print(f"x=3.5 시간일 때 합격 확률: {predicted_prob.item() * 100:.2f}%")
# 2.5시간 공부했을 때의 합격 확률 예측
new_input = torch.tensor([[2.5]])
predicted_prob = model(new_input)
print(f"x=2.5 시간일 때 합격 확률: {predicted_prob.item() * 100:.2f}%")
```
지금까지 우리는 딥러닝의 가장 기본 단위인 '선형 회귀'와 '로지스틱 회귀'를 알아봤습니다. 이 둘은 사실 **얕은 신경망(Shallow Neural Network)**이라고도 불립니다.