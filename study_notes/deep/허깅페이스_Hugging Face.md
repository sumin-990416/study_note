## í—ˆê¹…í˜ì´ìŠ¤ (Hugging Face)

**1. í—ˆê¹…í˜ì´ìŠ¤(Hugging Face)ë€?**

- íŠ¸ëœìŠ¤í¬ë¨¸(Transformer) ëª¨ë¸(ì˜ˆ: BERT, GPT)ì„ ëˆ„êµ¬ë‚˜ ì‰½ê²Œ ë‹¤ìš´ë¡œë“œí•˜ê³ , í•™ìŠµì‹œí‚¤ê³ , ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë§Œë“  **AI ëª¨ë¸ì˜ ê±°ëŒ€í•œ í—ˆë¸Œ(ì €ì¥ì†Œ)ì´ì ë¼ì´ë¸ŒëŸ¬ë¦¬**ì…ë‹ˆë‹¤.
    
- PyTorchë¡œ `nn.Transformer`ë¥¼ ì§ì ‘ ë§Œë“œëŠ” ê²ƒì´ 'ìë™ì°¨ ë¶€í’ˆìœ¼ë¡œ ì°¨ë¥¼ ì¡°ë¦½'í•˜ëŠ” ê²ƒì´ë¼ë©´, í—ˆê¹…í˜ì´ìŠ¤ëŠ” 'ì™„ì„±ëœ ìŠˆí¼ì¹´ì˜ í‚¤ë¥¼ ë°›ì•„ ë°”ë¡œ ìš´ì „'í•˜ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.
    

**2. í•µì‹¬ ê¸°ëŠ¥ 1: `pipeline` (ê°€ì¥ ì‰¬ìš´ ë°©ë²•)**

- í—ˆê¹…í˜ì´ìŠ¤ë¥¼ ì‹œì‘í•˜ëŠ” ê°€ì¥ ì‰½ê³  ë¹ ë¥¸ ë°©ë²•ì…ë‹ˆë‹¤.
    
- `pipeline`ì€ **"ì–´ë–¤ ì‘ì—…(task)"**ì„ í•˜ê³  ì‹¶ì€ì§€ë§Œ ë§í•´ì£¼ë©´, ì•Œì•„ì„œ ê·¸ ì‘ì—…ì— ê°€ì¥ ì í•©í•œ ëª¨ë¸ê³¼ ë°ì´í„° ì²˜ë¦¬ ë°©ì‹(í† í¬ë‚˜ì´ì €)ì„ **ìë™ìœ¼ë¡œ** ë‹¤ìš´ë¡œë“œí•˜ê³  ì‹¤í–‰í•´ ì¤ë‹ˆë‹¤.
    
- ìš°ë¦¬ê°€ êµì•ˆ 7ì—ì„œ `DataLoader`ë¡œ íŒŒì´í”„ë¼ì¸ì„ ì§°ë˜ ê²ƒì²˜ëŸ¼, `pipeline`ì€ **"ë°ì´í„° ì…ë ¥ $\rightarrow$ ëª¨ë¸ ì˜ˆì¸¡ $\rightarrow$ ê²°ê³¼ ì¶œë ¥"**ì˜ ì „ ê³¼ì •ì„ í•˜ë‚˜ë¡œ í•©ì³ì¤ë‹ˆë‹¤.
    
- **ì£¼ìš” `pipeline` ì‘ì—…(task) ì˜ˆì‹œ:**
    
    - `sentiment-analysis`: ê°ì„± ë¶„ì„ ("ì´ ë¬¸ì¥ì€ ê¸ì •ì¸ê°€ ë¶€ì •ì¸ê°€?")
        
    - `text-generation`: í…ìŠ¤íŠ¸ ìƒì„± (ì˜ˆ: GPT)
        
    - `fill-mask`: ë¹ˆì¹¸ ì±„ìš°ê¸° (ì˜ˆ: "ë‚˜ëŠ” [MASK]ì— ê°„ë‹¤.") (BERTì˜ ì£¼íŠ¹ê¸°)
        
    - `translation_xx_to_yy`: ë²ˆì—­

```python
# pip install transformers
import torch
# ğŸŒŸ í—ˆê¹…í˜ì´ìŠ¤ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ pipelineì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
from transformers import pipeline

# --- 1. íŒŒì´í”„ë¼ì¸ ìƒì„± ---
# "ê°ì„± ë¶„ì„(sentiment-analysis)" ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” íŒŒì´í”„ë¼ì¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.
# (ì´ë•Œ, í—ˆê¹…í˜ì´ìŠ¤ê°€ ì´ ì‘ì—…ì— ì í•©í•œ ê¸°ë³¸ ëª¨ë¸ì„ ì•Œì•„ì„œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.)
print("íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì¤‘...")
sentiment_pipeline = pipeline(task="sentiment-analysis")
print("ë¡œë“œ ì™„ë£Œ!")

# --- 2. ë°ì´í„° ì¤€ë¹„ ---
my_text1 = "This movie is so great and wonderful!"
my_text2 = "I hate this restaurant. The food was terrible."
my_texts = [my_text1, my_text2]

# --- 3. íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ëª¨ë¸ ì˜ˆì¸¡) ---
# ë³µì¡í•œ DataLoaderë‚˜ model(x)ê°€ í•„ìš” ì—†ìŠµë‹ˆë‹¤.
# ê·¸ëƒ¥ íŒŒì´í”„ë¼ì¸ì— ë°ì´í„°ë¥¼ ë„£ìœ¼ë©´ ëì…ë‹ˆë‹¤.
results = sentiment_pipeline(my_texts)

# --- 4. ê²°ê³¼ í™•ì¸ ---
print("\n--- ì˜ˆì¸¡ ê²°ê³¼ ---")
for text, result in zip(my_texts, results):
    print(f"ì…ë ¥: {text}")
    print(f"ê²°ê³¼: Label={result['label']}, Score={result['score']:.4f}")

# --- ì˜ˆì œ 2: ë¹ˆì¹¸ ì±„ìš°ê¸° (Fill-Mask) ---
print("\n--- ë¹ˆì¹¸ ì±„ìš°ê¸° íŒŒì´í”„ë¼ì¸ ---")
fill_mask_pipeline = pipeline(task="fill-mask")
text_with_mask = "The capital of France is [MASK]."
mask_results = fill_mask_pipeline(text_with_mask)

print(f"ì…ë ¥: {text_with_mask}")
for result in mask_results:
    # (scoreê°€ ê°€ì¥ ë†’ì€ ìƒìœ„ 5ê°œ ì •ë„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤)
    print(f"ì˜ˆì¸¡: {result['token_str']} (Score: {result['score']:.4f})")
```

`pipeline`ì€ ì •ë§ í¸ë¦¬í•˜ì§€ë§Œ, 'ë¸”ë™ ë°•ìŠ¤'ì™€ ê°™ì•„ì„œ ìš°ë¦¬ê°€ ê°€ì§„ ìƒˆë¡œìš´ ë°ì´í„°ë¡œ ëª¨ë¸ì„ _í•™ìŠµ_ì‹œí‚¬ ìˆ˜ëŠ” ì—†ì–´ìš”.

ê·¸ë ‡ê²Œ í•˜ë ¤ë©´, `pipeline`ì´ ìë™ìœ¼ë¡œ í•´ì¤¬ë˜ ë‘ ê°€ì§€ í•µì‹¬ ë‹¨ê³„ë¥¼ ìš°ë¦¬ê°€ ì§ì ‘ ìˆ˜í–‰í•´ì•¼ í•´ìš”.

1. **í† í¬ë‚˜ì´ì € (Tokenizer) ğŸ“–:** ëª¨ë¸(BERT, GPT ë“±)ì€ "ì•ˆë…•í•˜ì„¸ìš”" ê°™ì€ í…ìŠ¤íŠ¸ë¥¼ ì´í•´í•˜ì§€ ëª»í•´ìš”. ì˜¤ì§ ìˆ«ìë§Œ ì´í•´í•  ìˆ˜ ìˆì£ . **í† í¬ë‚˜ì´ì €**ëŠ” ìš°ë¦¬ì˜ í…ìŠ¤íŠ¸ë¥¼, ê·¸ ëª¨ë¸ì´ ì‚¬ì „ì— í•™ìŠµí–ˆë˜ ë°©ì‹ê³¼ **ë˜‘ê°™ì€** ìˆ«ì(í† í°) í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•´ ì£¼ëŠ” 'ë²ˆì—­ê¸°'ì…ë‹ˆë‹¤.
    
2. **ëª¨ë¸ (Model) ğŸ§ :** êµì•ˆ 10ì—ì„œ ë°°ìš´, ì‹¤ì œ ì‚¬ì „ í•™ìŠµëœ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸('ë‡Œ')ì…ë‹ˆë‹¤.
    

í—ˆê¹…í˜ì´ìŠ¤ì˜ ë§ˆë²•ì€ ë°”ë¡œ `Auto` í´ë˜ìŠ¤ì— ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ê·¸ì € í—ˆê¹…í˜ì´ìŠ¤ í—ˆë¸Œì— ìˆëŠ” ëª¨ë¸ì˜ ì´ë¦„(ID, ì˜ˆ: `"bert-base-uncased"`)ë§Œ ì•Œë ¤ì£¼ë©´, `AutoTokenizer`ì™€ `AutoModel`ì´ ì•Œì•„ì„œ ê·¸ ì´ë¦„ì— ë§ëŠ” 'í† í¬ë‚˜ì´ì €'ì™€ 'ëª¨ë¸' í•œ ìŒì„ ë‹¤ìš´ë¡œë“œí•´ ì¤ë‹ˆë‹¤.

---

## `AutoTokenizer` & `AutoModel` 

- ì´ ì½”ë“œëŠ” ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸(BERT)ì„ ìˆ˜ë™ìœ¼ë¡œ ë¡œë“œí•˜ê³  ì‚¬ìš©í•˜ì—¬ ì›ì‹œ ì¶œë ¥(raw output)ì„ ì–»ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
    
- (ì‹¤í–‰ ì „ `pip install transformers` í•„ìš”)

```python
import torch
# ğŸŒŸ AutoTokenizerì™€ AutoModelì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
from transformers import AutoTokenizer, AutoModel

# --- 1. ëª¨ë¸ ì´ë¦„(ID) ì§€ì • ---
# í—ˆê¹…í˜ì´ìŠ¤ Hubì—ì„œ ì‚¬ìš©í•  ëª¨ë¸ì˜ ì´ë¦„ì„ ì •í•©ë‹ˆë‹¤.
# "bert-base-uncased"ëŠ” ê°€ì¥ ê¸°ë³¸ì´ ë˜ëŠ” (ì˜ì–´) BERT ëª¨ë¸ì…ë‹ˆë‹¤.
# (ë§Œì•½ í•œê¸€ì„ ë‹¤ë£¨ê³  ì‹¶ë‹¤ë©´, "klue/bert-base" ê°™ì€ í•œê¸€ ëª¨ë¸ ì´ë¦„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤)
MODEL_NAME = "bert-base-uncased" 

# --- 2. Tokenizer ë¡œë“œ ---
# MODEL_NAMEì— í•´ë‹¹í•˜ëŠ” 'ë‹¨ì–´ì¥'ê³¼ 'í† í°í™” ê·œì¹™'ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
print(f"'{MODEL_NAME}'ì˜ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# --- 3. Model ë¡œë“œ ---
# MODEL_NAMEì— í•´ë‹¹í•˜ëŠ” 'ëª¨ë¸ êµ¬ì¡°'ì™€ 'ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜(íŒŒë¼ë¯¸í„°)'ë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
print(f"'{MODEL_NAME}'ì˜ ëª¨ë¸ ë¡œë“œ ì¤‘...")
model = AutoModel.from_pretrained(MODEL_NAME)
model.eval() # í•™ìŠµì´ ì•„ë‹Œ, ì¶”ë¡ (inference) ëª¨ë“œë¡œ ì„¤ì •

# --- 4. ë°ì´í„° ì¤€ë¹„ ë° í† í°í™” ---
my_texts = [
    "Hugging Face is a great library.",
    "Transformers are powerful."
]

print(f"\nì›ë³¸ í…ìŠ¤íŠ¸: {my_texts}")

# í† í¬ë‚˜ì´ì €ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
# padding=True: ë¬¸ì¥ ê¸¸ì´ë¥¼ ë§ì¶”ê¸° ìœ„í•´ ì§§ì€ ë¬¸ì¥ì— [PAD] í† í° ì¶”ê°€
# truncation=True: ëª¨ë¸ì´ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ ê¸¸ì´ë¥¼ ë„˜ìœ¼ë©´ ìë¥´ê¸°
# return_tensors='pt': ê²°ê³¼ë¥¼ PyTorch í…ì„œë¡œ ë°˜í™˜
inputs = tokenizer(my_texts, padding=True, truncation=True, return_tensors="pt")

# 'inputs'ëŠ” ë”•ì…”ë„ˆë¦¬(dictionary) í˜•íƒœì…ë‹ˆë‹¤.
# print(f"í† í°í™” ê²°ê³¼ (inputs): \n{inputs}")
# 'input_ids'ëŠ” í…ìŠ¤íŠ¸ê°€ ë³€í™˜ëœ ìˆ«ì í…ì„œì…ë‹ˆë‹¤.
print(f"í† í° ID (input_ids): \n{inputs['input_ids']}")

# --- 5. ëª¨ë¸ ì˜ˆì¸¡ (Forward Pass) ---
# ëª¨ë¸ì— í† í°í™”ëœ í…ì„œë¥¼ ë„£ìŠµë‹ˆë‹¤.
# torch.no_grad()ëŠ” "ë¯¸ë¶„(gradient) ê³„ì‚°ì„ í•˜ì§€ ì•Šê² ë‹¤" (ì¶”ë¡  ì‹œ ë©”ëª¨ë¦¬ ì ˆì•½)
with torch.no_grad():
    # **inputsëŠ” ë”•ì…”ë„ˆë¦¬ì˜ ëª¨ë“  í‚¤-ê°’ì„ (input_ids=..., attention_mask=...)ë¡œ
    # í’€ì–´ì„œ ëª¨ë¸ì— ì „ë‹¬í•˜ëŠ” Python ë¬¸ë²•ì…ë‹ˆë‹¤.
    outputs = model(**inputs)

# --- 6. ê²°ê³¼ í™•ì¸ ---
# 'outputs.last_hidden_state'ê°€ ëª¨ë¸ì˜ ìµœì¢… ì¶œë ¥ì…ë‹ˆë‹¤.
# ì´ê²ƒì´ ë°”ë¡œ í…ìŠ¤íŠ¸ì˜ 'ë¬¸ë§¥ì  ì˜ë¯¸'ê°€ ì••ì¶•ëœ ê³ ì°¨ì› ë²¡í„°ì…ë‹ˆë‹¤!
last_hidden_state = outputs.last_hidden_state

print(f"\nëª¨ë¸ ìµœì¢… ì¶œë ¥ (last_hidden_state)ì˜ Shape:")
# (Batch Size, Sequence Length, Hidden Dim)
# (ë°ì´í„° 2ê°œ, í† í° 8ê°œ, BERT-baseì˜ ì€ë‹‰ ì°¨ì› 768)
print(last_hidden_state.shape)
```


## ë¯¸ì„¸ì¡°ì • (Fine-Tuning)

**1. ë¯¸ì„¸ì¡°ì •(Fine-Tuning)ì´ë€?**

- **ê°œë…:** ì´ë¯¸ ìˆ˜ë°±ë§Œ, ìˆ˜ì‹­ì–µ ê°œì˜ í…ìŠ¤íŠ¸ë¡œ 'ì„¸ìƒì˜ ì¼ë°˜ì ì¸ ì§€ì‹'ì„ í•™ìŠµí•œ **ì‚¬ì „ í•™ìŠµëœ(pre-trained)** íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸(ì˜ˆ: BERT, GPT)ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    
- ê·¸ë¦¬ê³  ì´ ê±°ëŒ€í•œ ëª¨ë¸ì„ **ìš°ë¦¬ì˜ íŠ¹ì • ì‘ì—…**(ì˜ˆ: ì˜í™” ë¦¬ë·° ê¸/ë¶€ì • ë¶„ë¥˜, ìŠ¤íŒ¸ ë©”ì¼ ë¶„ë¥˜)ì— ë§ê²Œ **'ì‚´ì§' ì¶”ê°€ë¡œ í•™ìŠµ**ì‹œí‚¤ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.
    
- ë¹„ìœ : ì˜ì–´ ì›ì–´ë¯¼(ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸)ì—ê²Œ 'ì˜í•™ ìš©ì–´'(ìš°ë¦¬ ë°ì´í„°)ë¥¼ ê°€ë¥´ì³ì„œ 'ì˜í•™ ì „ë¬¸ ë²ˆì—­ê°€'(ë¯¸ì„¸ì¡°ì •ëœ ëª¨ë¸)ë¡œ ë§Œë“œëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.
    

**2. `AutoModel` vs. `AutoModelForSequenceClassification`**

- **êµì•ˆ 11:** `AutoModel`ì„ ì¼ìŠµë‹ˆë‹¤.
    
    - ì´ê±´ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ 'ëª¸í†µ'ë§Œ ê°€ì ¸ì˜¨ ê±°ì˜ˆìš”.
        
    - ì¶œë ¥ì€ `last_hidden_state`(ë¬¸ë§¥ì´í•´ ë²¡í„°)ì˜€ìŠµë‹ˆë‹¤.
        
    - ì´ê±¸ë¡œ ë¶„ë¥˜ë¥¼ í•˜ë ¤ë©´, ìš°ë¦¬ê°€ ì§ì ‘ `nn.Linear`ë¥¼ ë’¤ì— ë¶™ì—¬ì•¼ í–ˆì£ .
        
- **êµì•ˆ 13 (ì§€ê¸ˆ):** `AutoModelForSequenceClassification`ì„ ì”ë‹ˆë‹¤.
    
    - ì´ê²ƒì€ 'ëª¸í†µ' + 'ë¶„ë¥˜ ì‘ì—…ìš© ë¨¸ë¦¬(`nn.Linear`)'ê°€ **ì´ë¯¸ ê²°í•©ëœ** ì™„ì„±í’ˆ ëª¨ë¸ì…ë‹ˆë‹¤.
        
    - ìš°ë¦¬ëŠ” ê·¸ì € ì´ ëª¨ë¸ì—ê²Œ "ì •ë‹µì€ 2ê°œì•¼(ì˜ˆ: ê¸ì •/ë¶€ì •)"ë¼ê³  ì•Œë ¤ì£¼ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤.
        

**3. ë¯¸ì„¸ì¡°ì •ì˜ í•µì‹¬ ë„êµ¬: `Trainer` API**

- **êµì•ˆ 7:** ìš°ë¦¬ëŠ” ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ ë‹¤ë£¨ê¸° ìœ„í•´ `Dataset`ê³¼ `DataLoader`ë¥¼ ì§°ìŠµë‹ˆë‹¤.
    
- **êµì•ˆ 3~5:** ìš°ë¦¬ëŠ” ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ `for epoch...`ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” **'í•™ìŠµ ë£¨í”„(Training Loop)'**ë¥¼ ì§ì ‘ ì§°ìŠµë‹ˆë‹¤.
    
    - `optimizer.zero_grad()`
        
    - `loss.backward()`
        
    - `optimizer.step()`
        
- í—ˆê¹…í˜ì´ìŠ¤ì˜ `Trainer`ëŠ” ì´ ëª¨ë“  ê³¼ì •ì„ **ë‹¨ ì„¸ ì¤„**ë¡œ ì••ì¶•í•´ ì¤ë‹ˆë‹¤.
    
    1. `TrainingArguments` (í•™ìŠµ ì„¤ì •) ì •ì˜
        
    2. `Trainer` (ëª¨ë¸, ì„¤ì •, ë°ì´í„°) ì •ì˜
        
    3. `trainer.train()` (í•™ìŠµ ì‹œì‘)
        

---

## í—ˆê¹…í˜ì´ìŠ¤ `Trainer`ë¡œ ë¯¸ì„¸ì¡°ì •

- ì´ ì½”ë“œëŠ” ê°€ìƒì˜ ë°ì´í„°ë¡œ ì‹¤ì œ ë¯¸ì„¸ì¡°ì • í•™ìŠµì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
- **í•„ìˆ˜ ì„¤ì¹˜:** `pip install transformers datasets accelerate` (accelerateëŠ” Trainer ì‹¤í–‰ì— í•„ìš”)
```python
import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification, # ğŸŒŸ 'ì‘ì—…ìš©' ëª¨ë¸
    Trainer,                            # ğŸŒŸ í•™ìŠµ ë£¨í”„ ìë™í™”
    TrainingArguments                   # ğŸŒŸ í•™ìŠµ ì„¤ì •
)
from datasets import Dataset # ğŸŒŸ í—ˆê¹…í˜ì´ìŠ¤ ë°ì´í„°ì…‹ ë¼ì´ë¸ŒëŸ¬ë¦¬

# --- 0. ë°ì´í„° ì¤€ë¹„ (ê°€ìƒì˜ ì˜í™” ë¦¬ë·°) ---
# (ì‹¤ì œë¡œëŠ” .csv íŒŒì¼ì´ë‚˜ í—ˆê¹…í˜ì´ìŠ¤ Hubì—ì„œ ë°”ë¡œ ë¡œë“œí•©ë‹ˆë‹¤)
raw_data = {
    "text": [
        "This movie was fantastic!", 
        "I absolutely loved it.",
        "The acting was terrible.",
        "What a waste of time."
    ],
    # ğŸŒŸ 'label' (ì •ë‹µ): 1 (ê¸ì •), 0 (ë¶€ì •)
    "label": [1, 1, 0, 0] 
}
# ë”•ì…”ë„ˆë¦¬ë¥¼ í—ˆê¹…í˜ì´ìŠ¤ Dataset ê°ì²´ë¡œ ë³€í™˜
dataset = Dataset.from_dict(raw_data)

# --- 1. í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë“œ ---
MODEL_NAME = "bert-base-uncased" # (ì‘ì€ BERT ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸)

# 1-1. í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# 1-2. (ì¤‘ìš”!) ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ
model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME, 
    num_labels=2 # ğŸŒŸ "ì´ ëª¨ë¸ì€ 2ê°œ(0ê³¼ 1)ë¡œ ë¶„ë¥˜í•˜ëŠ” ëª¨ë¸ì´ì•¼"
)

# --- 2. ë°ì´í„° í† í°í™” ---
# ë°ì´í„°ì…‹ ì „ì²´ì— í† í¬ë‚˜ì´ì €ë¥¼ í•œ ë²ˆì— ì ìš©í•˜ëŠ” í•¨ìˆ˜
def tokenize_function(examples):
    # padding, truncationì„ ë™ì‹œì— ìˆ˜í–‰
    return tokenizer(examples["text"], padding="max_length", truncation=True)

# dataset.map()ì„ ì‚¬ìš©í•´ 'tokenize_function'ì„ ëª¨ë“  ë°ì´í„°ì— ì ìš©
tokenized_dataset = dataset.map(tokenize_function, batched=True)

# (í•™ìŠµì— í•„ìš”í•œ 'text' ì—´ì€ ì œê±°í•˜ê³  PyTorch í…ì„œë¡œ í¬ë§· ë³€ê²½)
tokenized_dataset = tokenized_dataset.remove_columns(["text"])
tokenized_dataset.set_format("torch")

# (ì‹¤ì œë¡œëŠ” train/validation setì„ ë‚˜ëˆ„ì§€ë§Œ ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ ì „ì²´ ì‚¬ìš©)
train_dataset = tokenized_dataset

# --- 3. í•™ìŠµ(Training) ì„¤ì • ---

# 3-1. TrainingArguments: í•™ìŠµì— í•„ìš”í•œ ëª¨ë“  ì„¤ì •ì„ ì •ì˜
training_args = TrainingArguments(
    output_dir="./results",      # ëª¨ë¸ê³¼ ë¡œê·¸ê°€ ì €ì¥ë  í´ë”
    num_train_epochs=3,          # ì´ í•™ìŠµ Epoch
    per_device_train_batch_size=2, # ë°°ì¹˜ ì‚¬ì´ì¦ˆ
    logging_steps=1,             # 1ë²ˆ stepë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥
)

# 3-2. Trainer: í•™ìŠµì„ ì‹¤í–‰í•  ì£¼ì²´
trainer = Trainer(
    model=model,                 # ğŸŒŸ ë¯¸ì„¸ì¡°ì •í•  ëª¨ë¸
    args=training_args,          # ğŸŒŸ í•™ìŠµ ì„¤ì •
    train_dataset=train_dataset, # ğŸŒŸ í•™ìŠµ ë°ì´í„°
    # (ì‹¤ì œë¡œëŠ” eval_dataset=... ë„ ë„£ì–´ì¤ë‹ˆë‹¤)
)

# --- 4. í•™ìŠµ ì‹œì‘! ---
print("--- ë¯¸ì„¸ì¡°ì •(Fine-Tuning) ì‹œì‘ ---")
trainer.train()
print("--- í•™ìŠµ ì™„ë£Œ ---")

# --- 5. í•™ìŠµëœ ëª¨ë¸ë¡œ ì˜ˆì¸¡í•´ë³´ê¸° ---
print("\n--- í•™ìŠµëœ ëª¨ë¸ë¡œ ì˜ˆì¸¡ ---")
test_text = "I really enjoyed this film."
inputs = tokenizer(test_text, return_tensors="pt") # í† í°í™”

# (ë¯¸ë¶„ ê³„ì‚° ì•ˆ í•¨)
with torch.no_grad():
    logits = model(**inputs).logits # ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ (raw score)

# ë¡œì§“(logits) ì¤‘ ê°€ì¥ ë†’ì€ ê°’ì˜ ì¸ë±ìŠ¤(0 ë˜ëŠ” 1)ë¥¼ ì°¾ìŒ
predicted_class_id = torch.argmax(logits, dim=1).item()
print(f"ì…ë ¥: {test_text}")
print(f"ì˜ˆì¸¡: {'ê¸ì •(1)' if predicted_class_id == 1 else 'ë¶€ì •(0)'}")
```