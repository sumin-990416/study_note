**1. 텐서(Tensor)란 무엇인가?**

- 간단히 말해, 딥러닝에서 사용하는 **다차원 배열(multi-dimensional array)**입니다.
    
- NumPy의 `ndarray`에 익숙하시다면, 텐서는 기본적으로 이와 거의 같다고 생각하시면 됩니다.
    
    - `[1, 2, 3]` → 1차원 텐서 (벡터)
        
    - `[[1, 2], [3, 4]]` → 2차원 텐서 (행렬)
        
    - (이후 이미지 데이터 등은 3, 4, 5차원 텐서로 확장됩니다.)

**2. 왜 NumPy `ndarray` 대신 텐서를 사용할까요?**

NumPy와 텐서는 매우 유사하지만, 텐서에는 딥러닝을 위한 두 가지 강력한 핵심 기능이 추가되었습니다.

- **GPU 가속 ⚡:** 텐서는 CPU뿐만 아니라 **GPU**에서도 직접 계산할 수 있습니다. 딥러닝의 엄청나게 많은 계산을 GPU가 병렬로 처리하여 속도를 비약적으로 향상시킵니다.
    
- **자동 미분 (Autograd) 🧠:** 텐서에 가해진 모든 연산을 추적하여, 딥러닝 모델이 '학습'하는 데 필수적인 **미분값(gradient)을 자동으로 계산**해 줍니다. (이것이 NumPy와의 가장 결정적인 차이입니다!)
    

**3. 텐서 생성하기 (Core Syntax)**

NumPy에서 `np.array()`를 사용했듯이, PyTorch에서는 `torch.tensor()`를 사용합니다.

```python 
import torch
import numpy as np

# 1. Python 리스트로 텐서 만들기
data_list = [[1, 2], [3, 4]]
my_tensor = torch.tensor(data_list)

# 2. NumPy 배열로 텐서 만들기 (상호 변환이 자유롭습니다)
np_array = np.array(data_list)
tensor_from_numpy = torch.from_numpy(np_array)

# 3. 텐서를 NumPy 배열로 변환하기
numpy_from_tensor = my_tensor.numpy()

# 텐서의 정보 확인 (NumPy와 동일)
# print(f"텐서의 내용: \n{my_tensor}")
# print(f"텐서의 모양 (Shape): {my_tensor.shape}")
# print(f"텐서의 데이터 타입 (Dtype): {my_tensor.dtype}")
```

## 자동 미분 (Autograd) 🧠

텐서가 딥러닝의 '데이터'라면, **자동 미분(Autograd)**은 딥러닝을 '학습'시키는 엔진입니다.

**1. 딥러닝의 학습 원리: 경사 하강법 (Gradient Descent)**

- 머신러닝(ML)에서 모델을 학습시킬 때, 우리는 **손실 함수(Loss Function)**라는 것을 사용해 "모델의 예측이 정답과 얼마나 틀렸는지"를 측정합니다.
    
- 학습의 목표는 이 **손실(오차)을 최소화**하는 것입니다.
    
- 손실을 최소화하기 위해, 모델을 이루는 수많은 **파라미터(가중치 $w$와 편향 $b$)**를 조금씩 조정해 나갑니다.
    
- 이때 "어느 방향으로, 얼마나" 조절해야 손실이 가장 빨리 줄어드는지 알려주는 신호가 바로 **미분(Gradient, 기울기)**입니다.
    
- 즉, $\text{손실 함수를 각 파라미터}(w, b)\text{로 미분}$한 값이 필요합니다.
    

**2. 자동 미분 (Autograd)이란?**

- 딥러닝 모델은 수백만, 수십억 개의 파라미터( $w, b$ )를 가질 수 있습니다. 이 복잡한 계산의 미분값을 손으로 계산하는 것은 불가능합니다.
    
- **Autograd**는 PyTorch 텐서에서 일어나는 모든 연산을 추적하고, 이 복잡한 연산들의 미분값을 **자동으로 계산**해 주는 시스템입니다.
    
- 이것이 딥러닝이 가능한 핵심 이유이자, NumPy와의 가장 큰 차이점입니다.
    

**3. Autograd의 작동 (개념)**

1. **추적 시작:** 텐서를 생성할 때 `requires_grad=True` 라고 설정하면, PyTorch는 이 텐서와 관련된 모든 연산을 추적하기 시작합니다. (모델의 파라미터는 기본적으로 이 값이 `True`로 설정됩니다.)
    
2. **연산 수행 (Forward Pass):** 이 텐서들을 이용해 모델의 예측값과 최종 손실값(Loss)을 계산합니다. PyTorch는 이 모든 계산 과정을 그래프(Computation Graph)로 기억합니다.
    
3. **미분 계산 (Backward Pass):** 최종 손실값에 대해 `.backward()` 라는 함수를 딱 한 번 호출하면, PyTorch가 기억해 둔 계산 그래프를 거꾸로 거슬러 올라가며(역전파, Backpropagation) 손실에 영향을 준 **모든 파라미터의 미분값(gradient)**을 자동으로 계산해 줍니다.
    
4. **파라미터 업데이트:** 계산된 미분값을 이용해 모델의 파라미터( $w, b$ )를 업데이트하여 모델을 개선합니다. (이것이 바로 '학습'입니다.)