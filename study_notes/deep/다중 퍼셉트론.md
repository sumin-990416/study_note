- **1. 선형/로지스틱 회귀의 한계**

- 지금까지 배운 선형 회귀와 로지스틱 회귀는 강력하지만, 한계가 명확합니다.
    
- 이 모델들은 데이터를 **단 하나의 직선 (또는 곡선)**으로만 나눌 수 있습니다. (이를 '선형적으로 분리 가능하다(Linearly Separable)'고 합니다.)
    
- 하지만 만약 데이터가 복잡하게 얽혀있다면 어떨까요?
    

**2. 대표적인 문제: XOR 문제**

- XOR 문제는 단 하나의 직선으로는 0과 1을 절대 나눌 수 없는 고전적인 문제입니다.
    
    - `[0, 0]` $\rightarrow$ `0`
        
    - `[0, 1]` $\rightarrow$ `1`
        
    - `[1, 0]` $\rightarrow$ `1`
        
    - `[1, 1]` $\rightarrow$ `0`
        
- 로지스틱 회귀 모델($H(x) = \sigma(Wx+b)$)은 이 문제를 풀 수 없습니다.
    

**3. 다층 퍼셉트론 (MLP)의 아이디어**

- "그럼 로지스틱 회귀 같은 모델을 여러 겹으로 쌓으면 되지 않을까?"
    
- 이것이 바로 **다층 퍼셉트론(Multi-Layer Perceptron, MLP)** 또는 **심층 신경망(Deep Neural Network, DNN)**의 핵심 아이디어입니다.
    
- 입력층(Input Layer)과 출력층(Output Layer) 사이에 한 개 이상의 **은닉층(Hidden Layer)**을 추가합니다.
    
- **구조:** 입력 $\rightarrow$ [은닉층 1] $\rightarrow$ [은닉층 2] $\rightarrow$ ... $\rightarrow$ 출력
    
- 이렇게 층을 깊게 쌓음으로써, 모델은 단순한 직선이 아닌 매우 복잡하고 비선형적인 경계선을 학습할 수 있습니다.
    

**4. 핵심 요소: 비선형 활성화 함수 (Non-linear Activation)**

- **매우 중요한 점:** 단순히 `nn.Linear` 층만 여러 개 쌓는 것은 아무 의미가 없습니다.
    
    - $H(x) = W_2(W_1x + b_1) + b_2$ ... 이 복잡해 보이는 식은 정리하면 결국 $H(x) = W'x + b'$ 형태의 또 다른 **선형 함수**일 뿐입니다.
        
- 각 선형 층(`nn.Linear`)을 통과한 직후, 결과를 '비선형(non-linear)적'으로 구부려주는 **활성화 함수(Activation Function)**를 반드시 추가해야 합니다.
    
- 이 '구부리는' 과정이 있어야만 층을 쌓는 의미가 생기고, 복잡한 패턴을 학습할 수 있습니다.
    

**5. 새로운 표준: ReLU 활성화 함수**

- 로지스틱 회귀에서는 출력층에서 0~1의 확률을 얻기 위해 **시그모이드(Sigmoid)**를 썼습니다.
    
- 하지만 MLP의 **은닉층(Hidden Layer)**에서는 시그모이드보다 훨씬 성능이 좋은 **ReLU(Rectified Linear Unit)** 함수를 표준으로 사용합니다.
    
- **ReLU란?**
    
    - 매우 간단합니다: "입력이 0보다 크면 그대로 통과시키고, 0보다 작으면 0으로 만든다."
        
- 수식:
    
    $$f(x) = \max(0, x)$$
    
- **특징:**
    
    - 계산이 매우 빠릅니다.
        
    - 시그모이드가 가진 고질적인 문제(Vanishing Gradient, 기울기 소실)를 해결하여 깊은 층을 쌓을 수 있게 해줍니다.
        

**6. MLP의 기본 구조 (요약)**

MLP는 **[선형 층 + 활성화 함수]** 세트를 여러 겹 쌓아 올린 것입니다.

- **입력층 $\rightarrow$ 은닉층 1:** `nn.Linear(...)` $\rightarrow$ `nn.ReLU()`
    
- **은닉층 1 $\rightarrow$ 은닉층 2:** `nn.Linear(...)` $\rightarrow$ `nn.ReLU()`
    
- ...
    
- **마지막 은닉층 $\rightarrow$ 출력층:** `nn.Linear(...)` $\rightarrow$ **(마지막 활성화 함수)**
    
    - (분류 문제라면 `nn.Sigmoid()` 또는 `nn.Softmax()`)
        
    - (회귀 문제라면 활성화 함수 없음)

## MLP PyTorch 코드 (XOR 문제 풀기)

- **XOR 데이터:**
    
    - `[0, 0]` $\rightarrow$ `0`
        
    - `[0, 1]` $\rightarrow$ `1`
        
    - `[1, 0]` $\rightarrow$ `1`
        
    - `[1, 1]` $\rightarrow$ `0`

```python
import torch
import torch.nn as nn
import torch.optim as optim

# --- 0. 데이터 준비 ---
# 입력(x)은 [0,0], [0,1], [1,0], [1,1] (2개의 특성)
x_train = torch.tensor([[0.0, 0.0], 
                        [0.0, 1.0], 
                        [1.0, 0.0], 
                        [1.0, 1.0]])
# 정답(y)은 0, 1, 1, 0
y_train = torch.tensor([[0.0], [1.0], [1.0], [0.0]])

# --- 1. 모델 정의 (MLP) ---
# nn.Sequential을 사용해 층을 순서대로 쌓습니다.
model = nn.Sequential(
    # 1. 입력층 -> 은닉층
    # 입력 특성 2개(x1, x2)를 받아 4개의 노드가 있는 은닉층으로 보냅니다.
    nn.Linear(2, 4),  
    # 2. 은닉층의 활성화 함수 (비선형 변환)
    nn.ReLU(),        
    
    # 3. 은닉층 -> 출력층
    # 4개의 노드를 가진 은닉층에서 1개의 출력(확률)을 만듭니다.
    nn.Linear(4, 1),
    # 4. 출력층의 활성화 함수 (0~1 사이 확률)
    nn.Sigmoid()      
)
# (참고: 입력 2 -> 은닉 4 -> 출력 1 구조입니다)

# --- 2. 손실 함수(Loss) 및 옵티마이저(Optimizer) 정의 ---
criterion = nn.BCELoss()  # 바이너리 분류
optimizer = optim.SGD(model.parameters(), lr=0.1) # 학습률(lr)을 조금 높였습니다.

# --- 3. 학습(Training) 실행 ---
# XOR는 꽤 어려운 문제라 학습을 많이 시켜야 합니다.
epochs = 10000 

for epoch in range(epochs + 1):
    
    # 1. (Forward) 모델 예측
    prediction = model(x_train)
    
    # 2. (Forward) 손실 계산
    loss = criterion(prediction, y_train)
    
    # 3. (Backward 준비) 미분값 초기화
    optimizer.zero_grad()
    
    # 4. (Backward) 자동 미분
    loss.backward()
    
    # 5. (Update) 파라미터 업데이트
    optimizer.step()
    
    if epoch % 1000 == 0:
        # 0.5를 기준으로 0 또는 1로 예측값을 변환
        predicted_classes = (prediction >= 0.5).float()
        accuracy = (predicted_classes == y_train).float().mean()
        print(f'Epoch {epoch:5d}/{epochs} | Loss: {loss.item():.6f} | Accuracy: {accuracy.item() * 100:.2f}%')

# --- 4. 학습 결과 확인 ---
print("\n--- 학습 완료 후 ---")
print(f"모델 예측 결과 (0.5 기준):")
final_prediction = (model(x_train) >= 0.5).float()
for i in range(4):
    print(f"Input: {x_train[i].tolist()} | 정답: {y_train[i].item()} | 예측: {final_prediction[i].item()}")
```

```python
#교안처럼 코드 수정
import torch
import torch.nn as nn
import torch.optim as optim

# --- 0. 데이터 준비 ---
# XOR 데이터:
# [0, 0] -> 0
# [0, 1] -> 1
# [1, 0] -> 1
# [1, 1] -> 0
# 입력(x)은 2개의 특성([x1, x2])
x_train = torch.tensor([[0.0, 0.0], 
                        [0.0, 1.0], 
                        [1.0, 0.0], 
                        [1.0, 1.0]])
# 정답(y)은 0, 1, 1, 0
y_train = torch.tensor([[0.0], [1.0], [1.0], [0.0]])


# --- 1. 모델 정의 (class 사용) ---
# H(x) = Sigmoid( W2 * ReLU(W1*x + b1) + b2 )
class MLPModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        # nn.Sequential을 사용해 층을 순서대로 정의
        self.layers = nn.Sequential(
            # 1. 입력층 -> 은닉층 (입력 2 -> 은닉 4)
            nn.Linear(input_dim, hidden_dim), 
            # 2. 은닉층의 활성화 함수
            nn.ReLU(),
            # 3. 은닉층 -> 출력층 (은닉 4 -> 출력 1)
            nn.Linear(hidden_dim, output_dim),
            # 4. 출력층의 활성화 함수 (0~1 사이 확률)
            nn.Sigmoid() 
        )
    
    def forward(self, x):
        # 정의된 self.layers에 x를 통과시킴
        return self.layers(x)

# 모델 인스턴스(객체) 생성
# 입력 2, 은닉 4, 출력 1
model = MLPModel(input_dim=2, hidden_dim=4, output_dim=1)


# --- 2. 손실 함수(Loss) 및 옵티마이저(Optimizer) 정의 ---
# 손실 함수로 바이너리 크로스 엔트로피(BCE)를 사용
criterion = nn.BCELoss()
# 옵티마이저로 SGD 사용 (학습률 0.1)
optimizer = optim.SGD(model.parameters(), lr=0.1) 


# --- 3. 학습(Training) 실행 ---
# XOR는 꽤 어려운 문제라 학습을 많이 시켜야 합니다.
epochs = 10000 

for epoch in range(epochs + 1):
    
    # 1. (Forward) 모델 예측 (0~1 사이의 확률값)
    prediction = model(x_train)
    
    # 2. (Forward) 손실(Loss) 계산
    loss = criterion(prediction, y_train)
    
    # 3. (Backward 준비) 미분값 초기화
    optimizer.zero_grad()
    
    # 4. (Backward) 자동 미분 실행
    loss.backward()
    
    # 5. (Update) 파라미터 업데이트
    optimizer.step()
    
    if epoch % 1000 == 0:
        # 0.5를 기준으로 0 또는 1로 예측값을 변환
        predicted_classes = (prediction >= 0.5).float()
        # 정확도 계산
        accuracy = (predicted_classes == y_train).float().mean()
        print(f'Epoch {epoch:5d}/{epochs} | Loss: {loss.item():.6f} | Accuracy: {accuracy.item() * 100:.2f}%')

# --- 4. 학습 결과 확인 ---
print("\n--- 학습 완료 후 ---")
print(f"모델 예측 결과 (0.5 기준):")
# 학습된 모델로 전체 x_train에 대한 예측을 다시 수행
final_prediction = (model(x_train) >= 0.5).float()
for i in range(4):
    print(f"Input: {x_train[i].tolist()} | 정답: {y_train[i].item()} | 예측: {final_prediction[i].item()}")
```