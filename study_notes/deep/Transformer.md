## íŠ¸ëœìŠ¤í¬ë¨¸ (Transformer)

**1. RNN/LSTMì˜ í•œê³„: ì™œ 'ìˆœì°¨ ì²˜ë¦¬'ê°€ ë¬¸ì œì¸ê°€?**

- LSTM(êµì•ˆ 9)ì€ ì¥ê¸° ê¸°ì–µ ë¬¸ì œë¥¼ 'ê²Œì´íŠ¸'ë¡œ í•´ê²°í•˜ë ¤ í–ˆì§€ë§Œ, ê·¼ë³¸ì ì¸ í•œê³„ê°€ ìˆì—ˆìŠµë‹ˆë‹¤.
    
- "ë‚˜ëŠ” ì–´ì œ... ì¹œêµ¬ë¥¼ ë§Œë‚¬ë‹¤"ë¥¼ ì²˜ë¦¬í•˜ë ¤ë©´, "ë‚˜" $\rightarrow$ "ëŠ”" $\rightarrow$ "ì–´" $\rightarrow$ "ì œ" ... í•œ ë‹¨ì–´ì”© **ìˆœì„œëŒ€ë¡œ** ê³„ì‚°í•´ì•¼ í•©ë‹ˆë‹¤.
    
- ì´ ë°©ì‹ì€ ë‘ ê°€ì§€ í° ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.
    
    1. **ë³‘ë ¬ ì²˜ë¦¬ ë¶ˆê°€ëŠ¥:** 100ë²ˆì§¸ ë‹¨ì–´ë¥¼ ê³„ì‚°í•˜ë ¤ë©´ 1~99ë²ˆì§¸ ê³„ì‚°ì´ ëë‚˜ì•¼ í•©ë‹ˆë‹¤. GPUì˜ ì¥ì (ë³‘ë ¬ ì²˜ë¦¬)ì„ ì‚´ë¦´ ìˆ˜ ì—†ì–´ **í•™ìŠµì´ ë§¤ìš° ëŠë¦½ë‹ˆë‹¤.**
        
    2. **ì—¬ì „í•œ ì •ë³´ ë³‘ëª©:** ì•„ë¬´ë¦¬ LSTMì´ë¼ë„, 100ë‹¨ì–´ ì „ì˜ ëª¨ë“  ì •ë³´ë¥¼ 'ê¸°ì–µ í†µë¡œ(Cell State)' í•˜ë‚˜ì— ì••ì¶•í•´ ì „ë‹¬í•˜ëŠ” ê²ƒì€ í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤.
        

**2. íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì•„ì´ë””ì–´: "Attention Is All You Need"**

- 2017ë…„, êµ¬ê¸€ì€ "Attention Is All You Need" (ì£¼ëª©(attention)ì´ë©´ ì¶©ë¶„í•˜ë‹¤)ë¼ëŠ” ì „ì„¤ì ì¸ ë…¼ë¬¸ì„ ë°œí‘œí•©ë‹ˆë‹¤.
    
- **í•µì‹¬ ì•„ì´ë””ì–´:** "ìˆœì„œëŒ€ë¡œ ë³¼ í•„ìš” ì—†ë‹¤! ë¬¸ì¥ ì „ì²´ë¥¼ í•œ ë²ˆì— ë³´ê³ , ì–´ë–¤ ë‹¨ì–´ê°€ ì„œë¡œì—ê²Œ 'ì£¼ëª©'í•´ì•¼ í•˜ëŠ”ì§€ ê³„ì‚°í•˜ì!"
    
- ì´ê²ƒì´ ë°”ë¡œ **ì…€í”„-ì–´í…ì…˜ (Self-Attention)** ë©”ì»¤ë‹ˆì¦˜ì…ë‹ˆë‹¤.
    

**3. ì…€í”„-ì–´í…ì…˜ (Self-Attention)ì´ë€?**

- RNNì²˜ëŸ¼ 'ê¸°ì–µ'ì„ ì „ë‹¬í•˜ëŠ” ëŒ€ì‹ , ë¬¸ì¥ì˜ **ëª¨ë“  ë‹¨ì–´ê°€ ë‹¤ë¥¸ ëª¨ë“  ë‹¨ì–´ë¥¼ ì§ì ‘ ë´…ë‹ˆë‹¤.**
    
- ì˜ˆì‹œ: "The animal didn't cross the street because **it** was too tired."
    
- ëª¨ë¸ì€ "it"ì´ë¼ëŠ” ë‹¨ì–´ë¥¼ ê³„ì‚°í•  ë•Œ, ë¬¸ì¥ ì „ì²´("The", "animal", "didn't", ..., "tired")ë¥¼ **ë™ì‹œì—** ë´…ë‹ˆë‹¤.
    
- ê·¸ë¦¬ê³  "it"ì´ "The"ë‚˜ "street"ê°€ ì•„ë‹Œ **"animal"**ê³¼ ê°€ì¥ ê´€ë ¨ì´ ê¹Šë‹¤ëŠ” ê²ƒì„ **ì§ì ‘ ê³„ì‚°**í•´ëƒ…ë‹ˆë‹¤.
    

- ì´ ê³„ì‚°ì€ ìˆœì„œê°€ í•„ìš” ì—†ê³ , ë¬¸ì¥ ì „ì²´ì— ëŒ€í•´ **í•œ ë²ˆì— ë³‘ë ¬ë¡œ** ì¼ì–´ë‚©ë‹ˆë‹¤. (ë§¤ìš° ë¹ ë¦„!)
    

**4. íŠ¸ëœìŠ¤í¬ë¨¸ì˜ êµ¬ì¡°: Encoder-Decoder**

- íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” í¬ê²Œ ë‘ ë¶€ë¶„ìœ¼ë¡œ ë‚˜ë‰©ë‹ˆë‹¤. (ì£¼ë¡œ ë²ˆì—­ì„ ìœ„í•´ ì„¤ê³„ë¨)
    
    1. **ì¸ì½”ë” (Encoder):** ì…ë ¥ ë¬¸ì¥(ì˜ˆ: í•œêµ­ì–´)ì„ ë°›ì•„ì„œ, ë‹¨ì–´ë“¤ì˜ 'ê´€ê³„(attention)'ë¥¼ íŒŒì•…í•˜ê³  ë¬¸ë§¥ì  ì˜ë¯¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
    2. **ë””ì½”ë” (Decoder):** ì¸ì½”ë”ê°€ ì¶”ì¶œí•œ ì˜ë¯¸ë¥¼ ë°›ì•„ì„œ, ì¶œë ¥ ë¬¸ì¥(ì˜ˆ: ì˜ì–´)ì„ í•œ ë‹¨ì–´ì”© ìƒì„±í•´ëƒ…ë‹ˆë‹¤.
        
- **ì ê¹! ìˆœì„œê°€ ì—†ë‹¤ë©´? (Positional Encoding)**
    
    - "I am a boy"ì™€ "boy am I a"ë¥¼ êµ¬ë¶„í•´ì•¼ í•©ë‹ˆë‹¤.
        
    - íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” 'ìˆœì„œ' ì •ë³´ë¥¼ ìƒì–´ë²„ë ¸ê¸° ë•Œë¬¸ì—,

## íŠ¸ëœìŠ¤í¬ë¨¸ PyTorch ì½”ë“œ (ì „ì²´ ìŠ¤í¬ë¦½íŠ¸ Ver.)

**ê²½ê³ :** ì´ ì½”ë“œëŠ” PyTorchì˜ `nn.Transformer` ëª¨ë“ˆì„ ì§ì ‘ ì‚¬ìš©í•˜ëŠ” 'ë‚ ê²ƒ'ì˜ ì½”ë“œì…ë‹ˆë‹¤. **ë§¤ìš° ë³µì¡í•˜ë©°** ê°œë…ì„ ì„¤ëª…í•˜ê¸° ìœ„í•œ ê²ƒì…ë‹ˆë‹¤. (í—ˆê¹…í˜ì´ìŠ¤ë¥¼ ì“°ë©´ ì´ ëª¨ë“  ê²Œ í•œ ì¤„ë¡œ ëë‚©ë‹ˆë‹¤.)

- **ë¬¸ì œ:** (ê°€ìƒ) `[10, 20, 30, 40, 50]`ì´ë¼ëŠ” ìˆœì°¨ ì…ë ¥ì„ ë°›ì•„ì„œ `[12, 22, 32, 42, 52]` (+2 í•˜ëŠ” ê·œì¹™)ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì‹œí€€ìŠ¤-íˆ¬-ì‹œí€€ìŠ¤(Seq2Seq) ë¬¸ì œ.

```python
import torch
import torch.nn as nn
import torch.optim as optim
import math

# --- 1. ëª¨ë¸ ì •ì˜ (class ì‚¬ìš©) ---

# íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ìˆœì„œ ê°œë…ì´ ì—†ìœ¼ë¯€ë¡œ, 'ìœ„ì¹˜' ì •ë³´ë¥¼ ì¸ìœ„ì ìœ¼ë¡œ ë”í•´ì¤˜ì•¼ í•©ë‹ˆë‹¤.
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        # sin, cos í•¨ìˆ˜ë¥¼ ì´ìš©í•´ ìœ„ì¹˜ë³„ ê³ ìœ  ë²¡í„°ë¥¼ ë§Œë“­ë‹ˆë‹¤.
        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        pe = torch.zeros(max_len, 1, d_model)
        pe[:, 0, 0::2] = torch.sin(position * div_term)
        pe[:, 0, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)

    def forward(self, x):
        # ì…ë ¥ x (Batch, Seq, Dim)ì— ìœ„ì¹˜ ì •ë³´(pe)ë¥¼ ë”í•©ë‹ˆë‹¤.
        # x.size(1)ì€ ì‹œí€€ìŠ¤ ê¸¸ì´
        return x + self.pe[:x.size(1)].transpose(0, 1)

class TransformerModel(nn.Module):
    def __init__(self, input_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward):
        super().__init__()
        self.d_model = d_model
        
        # 1. ì…ë ¥ ì„ë² ë”© (1ì°¨ì› ìˆ«ìë¥¼ -> d_model ì°¨ì› ë²¡í„°ë¡œ)
        self.input_embedding = nn.Linear(input_dim, d_model)
        # 2. ìœ„ì¹˜ ì¸ì½”ë”©
        self.pos_encoder = PositionalEncoding(d_model)
        
        # 3. PyTorchì˜ Transformer í•µì‹¬ ëª¨ë“ˆ
        self.transformer = nn.Transformer(
            d_model=d_model,
            nhead=nhead, # ì…€í”„-ì–´í…ì…˜ì„ ëª‡ ê°œë¡œ ìª¼ê°œì„œ ë³¼ì§€ (Multi-Head Attention)
            num_encoder_layers=num_encoder_layers,
            num_decoder_layers=num_decoder_layers,
            dim_feedforward=dim_feedforward,
            batch_first=True # (ì¤‘ìš”!) ì…ë ¥ì„ (Batch, Seq, Dim) ìˆœì„œë¡œ
        )
        
        # 4. ìµœì¢… ì¶œë ¥ (d_model ì°¨ì› ë²¡í„°ë¥¼ -> 1ì°¨ì› ìˆ«ìë¡œ)
        self.fc_out = nn.Linear(d_model, input_dim)

    def forward(self, src, tgt):
        # src: ì¸ì½”ë” ì…ë ¥ (Batch, Src_Seq, Dim)
        # tgt: ë””ì½”ë” ì…ë ¥ (Batch, Tgt_Seq, Dim)
        
        # 1. ì„ë² ë”© ë° ìœ„ì¹˜ ì¸ì½”ë”©
        src = self.pos_encoder(self.input_embedding(src) * math.sqrt(self.d_model))
        tgt = self.pos_encoder(self.input_embedding(tgt) * math.sqrt(self.d_model))

        # 2. ë§ˆìŠ¤í¬ ìƒì„± (Masking)
        # (TransformerëŠ” ì •ë‹µì„ ë¯¸ë¦¬ ë³´ë©´ ì•ˆ ë˜ë¯€ë¡œ, 'ë¯¸ë˜'ë¥¼ ê°€ë ¤ì£¼ëŠ” ë§ˆìŠ¤í¬ê°€ í•„ìš”)
        # (ì´ ë¶€ë¶„ì´ ë§¤ìš° ë³µì¡í•œ ë¶€ë¶„ì…ë‹ˆë‹¤)
        tgt_mask = self.transformer.generate_square_subsequent_mask(tgt.size(1)).to(src.device)
        
        # 3. íŠ¸ëœìŠ¤í¬ë¨¸ ì‹¤í–‰
        output = self.transformer(src, tgt, tgt_mask=tgt_mask)
        
        # 4. ìµœì¢… ì¶œë ¥
        output = self.fc_out(output)
        return output

# --- 0. ë°ì´í„° ì¤€ë¹„ ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ---
# (Batch, Seq_len, Input_dim)
src = torch.tensor([[[10.], [20.], [30.], [40.], [50.]]]) # (1, 5, 1)
# ë””ì½”ë” ì…ë ¥(tgt)ì€ ì •ë‹µ(target)ì„ í•œ ì¹¸ì”© ë¯¼ ê²ƒ (shifted right)
# (ì˜ˆì¸¡ ì‹œì‘ì„ ìœ„í•œ <SOS> í† í°(0.) + ì •ë‹µì˜ 1~4ë²ˆì§¸)
tgt = torch.tensor([[[0.], [12.], [22.], [32.], [42.]]]) # (1, 5, 1)
# ëª¨ë¸ì´ ì˜ˆì¸¡í•´ì•¼ í•  ì‹¤ì œ ì •ë‹µ (target)
target = torch.tensor([[[12.], [22.], [32.], [42.], [52.]]]) # (1, 5, 1)

# í•˜ì´í¼íŒŒë¼ë¯¸í„° (ëª¨ë¸ì˜ í¬ê¸°ë¥¼ ë§¤ìš° ì‘ê²Œ ì„¤ì •)
input_dim = 1
d_model = 16       # ì„ë² ë”© ì°¨ì› (ëª¨ë¸ì˜ ì£¼ ì°¨ì›)
nhead = 2          # ë©€í‹° í—¤ë“œ ì–´í…ì…˜ ê°œìˆ˜
num_encoder_layers = 2
num_decoder_layers = 2
dim_feedforward = 32 # MLPì˜ hidden_dim

model = TransformerModel(input_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward)

# --- 2. ì†ì‹¤ í•¨ìˆ˜(Loss) ë° ì˜µí‹°ë§ˆì´ì €(Optimizer) ì •ì˜ ---
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# --- 3. í•™ìŠµ(Training) ì‹¤í–‰ ---
epochs = 500

for epoch in range(epochs + 1):
    
    # 1. (Forward) ëª¨ë¸ ì˜ˆì¸¡
    prediction = model(src, tgt) # srcì™€ tgtë¥¼ ëª¨ë‘ ë„£ì–´ì¤Œ
    
    # 2. (Forward) ì†ì‹¤ ê³„ì‚°
    # ì˜ˆì¸¡ ê²°ê³¼(prediction)ì™€ ì‹¤ì œ ì •ë‹µ(target) ë¹„êµ
    loss = criterion(prediction, target)
    
    # 3, 4, 5ë‹¨ê³„ (Backward)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if epoch % 50 == 0:
        print(f'Epoch {epoch:4d}/{epochs} | Loss: {loss.item():.6f}')

# --- 4. í•™ìŠµ ê²°ê³¼ í™•ì¸ ---
print("\n--- í•™ìŠµ ì™„ë£Œ í›„ ---")
model.eval() # í‰ê°€ ëª¨ë“œ
with torch.no_grad():
    prediction = model(src, tgt)
    print(f"ì…ë ¥: {src.view(-1).tolist()}")
    print(f"ì •ë‹µ: {target.view(-1).tolist()}")
    print(f"ì˜ˆì¸¡: {prediction.view(-1).tolist()}")
```

ë‹¤í–‰íˆë„, ìš°ë¦¬ê°€ ì›ë˜ ëª©í‘œí–ˆë˜ **í—ˆê¹…í˜ì´ìŠ¤(Hugging Face) ğŸ¤—**ëŠ” ì´ ëª¨ë“  ë³µì¡í•œ ê³¼ì •ì„ `pipeline`ì´ë‚˜ `AutoModel` ê°™ì€ ëª…ë ¹ì–´ë¡œ **ë‹¨ ëª‡ ì¤„** ë§Œì— ì²˜ë¦¬í•´ ì¤ë‹ˆë‹¤.

---
êµì•ˆ 10ì—ì„œ ìš°ë¦¬ëŠ” ì…€í”„-ì–´í…ì…˜ì´ "ë¬¸ì¥ ì•ˆì˜ ë‹¨ì–´ë“¤ì´ ì„œë¡œì˜ ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ì‹"ì´ë¼ê³  ë°°ì› ìŠµë‹ˆë‹¤. "The animal didn't cross the street because **it** was too tired."ë¼ëŠ” ë¬¸ì¥ì—ì„œ, ëª¨ë¸ì´ 'it'ì´ 'street'ê°€ ì•„ë‹ˆë¼ 'animal'ì— ì£¼ëª©í•œë‹¤ëŠ” ê²ƒì„ ì•ˆë‹¤ê³  í–ˆì£ .

ê·¸ë ‡ë‹¤ë©´ ëª¨ë¸ì€ 'it'ì´ 'street'ê°€ ì•„ë‹ˆë¼ 'animal'ì— ì£¼ëª©í•´ì•¼ í•œë‹¤ëŠ” ê²ƒì„ **ì–´ë–»ê²Œ** ì•Œ ìˆ˜ ìˆì„ê¹Œìš”?

ì—¬ê¸°ì„œ ë°”ë¡œ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ê°€ì¥ í•µì‹¬ì ì¸ ê°œë…ì¸ **Query(ì¿¼ë¦¬), Key(í‚¤), Value(ë°¸ë¥˜)**, ì¤„ì—¬ì„œ **Q, K, V**ê°€ ë“±ì¥í•©ë‹ˆë‹¤. ğŸ—ï¸

---

### ì…€í”„-ì–´í…ì…˜ì˜ ì‘ë™ ì›ë¦¬: Q, K, V

ì…€í”„-ì–´í…ì…˜ì„ ë„ì„œê´€ì—ì„œ ë‚´ê°€ í•„ìš”í•œ ì •ë³´ë¥¼ ì°¾ëŠ” ê³¼ì •ì´ë¼ê³  ìƒìƒí•´ ë³´ì„¸ìš”.

1. **Query (Q) ğŸ’¡ (ë‚˜ì˜ ì§ˆë¬¸):**
    
    - ë‚´ê°€ ì§€ê¸ˆ ì²˜ë¦¬í•˜ë ¤ëŠ” ë‹¨ì–´(ì˜ˆ: 'it')ê°€ ë¬¸ì¥ì˜ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì—ê²Œ ë˜ì§€ëŠ” 'ì§ˆë¬¸'ì…ë‹ˆë‹¤.
        
    - "ë‚˜ëŠ” ì´ ë¬¸ì¥ì—ì„œ ëˆ„êµ¬ë¥¼ ê°€ë¦¬í‚¤ëŠ” ê±°ì§€? ë‚˜ì™€ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì¤˜!"
        
2. **Key (K) ğŸ·ï¸ (ì •ë³´ì˜ ê¼¬ë¦¬í‘œ):**
    
    - ë¬¸ì¥ ì•ˆì˜ **ëª¨ë“  ë‹¨ì–´**(ì˜ˆ: 'The', 'animal', 'street', 'it' ìì‹  í¬í•¨)ê°€ ê°ê° ê°€ì§€ê³  ìˆëŠ” 'ê¼¬ë¦¬í‘œ' ë˜ëŠ” 'ìƒ‰ì¸'ì…ë‹ˆë‹¤.
        
    - "ë‚˜ëŠ” 'animal'ì´ê³ , ë¬¸ë§¥ìƒ 'ì£¼ì–´' ì—­í• ì„ í•´."
        
    - "ë‚˜ëŠ” 'street'ì´ê³ , ë¬¸ë§¥ìƒ 'ì¥ì†Œ'ë¥¼ ì˜ë¯¸í•´."
        
3. **Value (V) ğŸ“– (ì •ë³´ì˜ ì‹¤ì œ ë‚´ìš©):**
    
    - ê·¸ ë‹¨ì–´(ì˜ˆ: 'animal', 'street')ê°€ ì‹¤ì œë¡œ ë‹´ê³  ìˆëŠ” 'ë‚´ìš©' ë˜ëŠ” 'ì˜ë¯¸' ê·¸ ìì²´ì…ë‹ˆë‹¤.
        

---

### ì–´í…ì…˜ ê³„ì‚° ê³¼ì •

ëª¨ë¸ì€ 'it'ì´ë¼ëŠ” ë‹¨ì–´ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ë‹¤ìŒ 3ë‹¨ê³„ë¥¼ ê±°ì¹©ë‹ˆë‹¤.

1. **[1ë‹¨ê³„] ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° (Qì™€ Kì˜ ë§Œë‚¨):**
    
    - 'it'ì˜ **Query(Q)** ë²¡í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        
    - ì´ Që¥¼ ë¬¸ì¥ì˜ **ëª¨ë“ ** ë‹¨ì–´ì˜ **Key(K)** ë²¡í„°ì™€ í•˜ë‚˜ì”© ë¹„êµí•©ë‹ˆë‹¤. (ìˆ˜í•™ì ìœ¼ë¡œëŠ” 'ë‚´ì (Dot Product)'ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.)
        
    - 'it'(Q) â†” 'The'(K) $\rightarrow$ ì ìˆ˜: 5
        
    - 'it'(Q) â†” 'animal'(K) $\rightarrow$ ì ìˆ˜: **95** (ê°€ì¥ ë†’ìŒ!)
        
    - 'it'(Q) â†” 'street'(K) $\rightarrow$ ì ìˆ˜: 12
        
    - ...
        
    - ì´ê²ƒì´ ë°”ë¡œ 'it'ì´ 'animal'ê³¼ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ë‹¤ê³  íŒë‹¨í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤!
        
2. **[2ë‹¨ê³„] ê°€ì¤‘ì¹˜ ê³„ì‚° (Softmax):**
    
    - ìœ„ì—ì„œ ì–»ì€ ë‚ ê²ƒì˜ ì ìˆ˜(5, 95, 12...)ë¥¼ **ì†Œí”„íŠ¸ë§¥ìŠ¤(Softmax)** í•¨ìˆ˜ì— í†µê³¼ì‹œì¼œ, ì´í•©ì´ 1ì´ ë˜ëŠ” 'í™•ë¥ ' ë˜ëŠ” 'ê°€ì¤‘ì¹˜'ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        
    - 'The': 0.01 (1%)
        
    - 'animal': **0.88** (88%)
        
    - 'street': 0.03 (3%)
        
    - ...
        
3. **[3ë‹¨ê³„] ì˜ë¯¸ ì¶”ì¶œ (Vì™€ ê°€ì¤‘ì¹˜ ê³±):**
    
    - ì´ì œ ì´ ê°€ì¤‘ì¹˜ë¥¼ ê° ë‹¨ì–´ì˜ **Value(V)** ë²¡í„°ì— ê³±í•´ì„œ ëª¨ë‘ ë”í•©ë‹ˆë‹¤.
        
    - $($'The'ì˜ Value $\times$ 0.01$)$ + $($**'animal'ì˜ Value** $\times$ **0.88**$)$ + $($'street'ì˜ Value $\times$ 0.03$)$ + ...
        
    - **ê²°ê³¼:** 'it'ì˜ ìµœì¢…ì ì¸ ì˜ë¯¸ ë²¡í„°ëŠ” **'animal'ì˜ ì˜ë¯¸(Value)ë¥¼ 88%ë‚˜ ë°˜ì˜**í•œ ìƒˆë¡œìš´ ë²¡í„°ê°€ ë©ë‹ˆë‹¤.
        

ì´ê²ƒì´ ì…€í”„-ì–´í…ì…˜ì´ 'it'ì´ 'animal'ì„ ì°¸ê³ í•˜ë„ë¡ ë§Œë“œëŠ” ë°©ì‹ì…ë‹ˆë‹¤.

---

### ì…€í”„-ì–´í…ì…˜ ìˆ˜ì‹

ì´ Q, K, Vë¥¼ ì´ìš©í•œ ì „ì²´ ê³„ì‚° ê³¼ì •ì„ ì••ì¶•í•œ íŠ¸ëœìŠ¤í¬ë¨¸ ë…¼ë¬¸ì˜ ê·¸ ìœ ëª…í•œ ìˆ˜ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

- $QK^T$ : [1ë‹¨ê³„] 'it'ì˜ Qê°€ ëª¨ë“  ë‹¨ì–´ì˜ Kì™€ ë§Œë‚˜ ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.
    
- $\sqrt{d_k}$ : ì ìˆ˜ ê°’ì„ ì•ˆì •í™”ì‹œí‚¤ê¸° ìœ„í•œ ìŠ¤ì¼€ì¼ë§(scaling) ì‘ì—…ì…ë‹ˆë‹¤.
    
- $\text{softmax}(\dots)$ : [2ë‹¨ê³„] ì ìˆ˜ë¥¼ 0~1 ì‚¬ì´ì˜ ê°€ì¤‘ì¹˜ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
- $(\dots)V$ : [3ë‹¨ê³„] ê³„ì‚°ëœ ê°€ì¤‘ì¹˜ë¥¼ ì‹¤ì œ Value(ì˜ë¯¸)ì— ê³±í•´ ìµœì¢… ê²°ê³¼ë¥¼ ì–»ìŠµë‹ˆë‹¤.


ìˆ˜ì‹ $\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$ ë¥¼ PyTorch ì½”ë“œë¡œ í•¨ê»˜ ë‹¨ê³„ë³„ë¡œ ë§Œë“¤ì–´ ë³´ì£ . ğŸš€

ì „ì²´ ëª¨ë¸ í´ë˜ìŠ¤(`nn.Module`)ë¥¼ í•œ ë²ˆì— ë§Œë“œëŠ” ê²ƒë³´ë‹¤, **ë°ì´í„°ê°€ ì´ ìˆ˜ì‹ì„ ì–´ë–»ê²Œ í†µê³¼í•˜ëŠ”ì§€** ìˆœì„œëŒ€ë¡œ ì‚´í´ë³´ëŠ” ê²ƒì´ ì´í•´í•˜ê¸° í›¨ì”¬ ì¢‹ì•„ìš”.

### ì…€í”„-ì–´í…ì…˜ ì½”ë“œ êµ¬í˜„ (ë‹¨ê³„ë³„)


```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

# --- 0. ê°€ìƒì˜ ì…ë ¥ ë°ì´í„° ---
# (ë°°ì¹˜ í¬ê¸°=1, ì‹œí€€ìŠ¤ ê¸¸ì´=3, ì„ë² ë”© ì°¨ì›=4)
# 3ê°œì˜ ë‹¨ì–´ê°€ 4ì°¨ì› ë²¡í„°ë¡œ í‘œí˜„ëœ ìƒíƒœë¼ê³  ê°€ì •í•©ë‹ˆë‹¤.
x = torch.rand(1, 3, 4) 
print(f"Original Input x shape: {x.shape}")

# --- 1. Q, K, V ìƒì„±ì„ ìœ„í•œ Linear ì¸µ ---
embed_dim = 4 # ì„ë² ë”© ì°¨ì›
d_k = 4       # Keyì˜ ì°¨ì› (ì„¤ëª…ì„ ìœ„í•´ embed_dimê³¼ ê°™ê²Œ ì„¤ì •)
d_v = 4       # Valueì˜ ì°¨ì›

# (ì‹¤ì œ íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œëŠ” d_k, d_vê°€ embed_dim / num_heads ì…ë‹ˆë‹¤)
W_q = nn.Linear(embed_dim, d_k)
W_k = nn.Linear(embed_dim, d_k)
W_v = nn.Linear(embed_dim, d_v)

# --- 2. Q, K, V ë²¡í„° ìƒì„± ---
Q = W_q(x)  # shape: (1, 3, 4)
K = W_k(x)  # shape: (1, 3, 4)
V = W_v(x)  # shape: (1, 3, 4)
print(f"Q, K, V shapes: {Q.shape}, {K.shape}, {V.shape}")

# --- 3. ì…€í”„-ì–´í…ì…˜ ìˆ˜ì‹ êµ¬í˜„ ---

# 3-1. QK^T (ì ìˆ˜ ê³„ì‚°)
# Q (1, 3, 4) @ K.T (1, 4, 3) -> scores (1, 3, 3)
scores = torch.matmul(Q, K.transpose(-2, -1))
print(f"\n1. Scores (QK^T) shape: {scores.shape}")

# 3-2. Scaling (ì ìˆ˜ ì•ˆì •í™”)
# scores (1, 3, 3) / sqrt(d_k)
scaled_scores = scores / math.sqrt(d_k)
print(f"2. Scaled Scores (scores / sqrt(d_k)) shape: {scaled_scores.shape}")

# 3-3. Softmax (ê°€ì¤‘ì¹˜ ë³€í™˜)
# dim=-1 (í˜¹ì€ dim=2) : ë§ˆì§€ë§‰ ì°¨ì›(ê¸¸ì´ 3ì§œë¦¬)ì— ëŒ€í•´ ì†Œí”„íŠ¸ë§¥ìŠ¤ë¥¼ ì ìš©
# ì¦‰, (1, 3, [ì—¬ê¸°]) -> ê° í–‰ì˜ í•©ì´ 1ì´ ë˜ë„ë¡ ë§Œë“­ë‹ˆë‹¤.
attention_weights = F.softmax(scaled_scores, dim=-1)
print(f"3. Attention Weights (softmax) shape: {attention_weights.shape}")
# print(f"ì²« ë²ˆì§¸ ë‹¨ì–´ì˜ ê°€ì¤‘ì¹˜ í•©: {attention_weights[0, 0, :].sum()}") # 1.0

# 3-4. ìµœì¢… Vì™€ ê³±í•˜ê¸° (ì˜ë¯¸ ê²°í•©)
# (1, 3, 3) @ (1, 3, 4) -> (1, 3, 4)
# ê°€ì¤‘ì¹˜ì™€ V(ì‹¤ì œ ì˜ë¯¸)ë¥¼ ê³±í•©ë‹ˆë‹¤.
output = torch.matmul(attention_weights, V)
print(f"4. Final Output (Weights @ V) shape: {output.shape}")


print("\n--- ìµœì¢… ê²°ê³¼ ---")
print(f"ì…ë ¥ x shape: {x.shape}")
print(f"ì¶œë ¥ output shape: {output.shape}")
```

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

# --- 1. ì…€í”„-ì–´í…ì…˜ 'ë ˆì´ì–´' ì •ì˜í•˜ê¸° ---
# ìš°ë¦¬ê°€ ë§Œë“  ë¡œì§ì„ nn.Module í´ë˜ìŠ¤ë¡œ í¬ì¥í•©ë‹ˆë‹¤.
class SelfAttentionHead(nn.Module):
    def __init__(self, embed_dim, d_k, d_v):
        """
        __init__: í•™ìŠµì´ í•„ìš”í•œ 'ê°€ì¤‘ì¹˜'(ë ˆì´ì–´)ë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤.
        """
        super().__init__()
        self.d_k = d_k
        
        # Q, K, Vë¥¼ ë§Œë“œëŠ” Linear ì¸µë“¤ (ì´ê²ƒë“¤ì´ í•™ìŠµë©ë‹ˆë‹¤)
        self.W_q = nn.Linear(embed_dim, d_k)
        self.W_k = nn.Linear(embed_dim, d_k)
        self.W_v = nn.Linear(embed_dim, d_v)

    def forward(self, x):
        """
        forward: ë°ì´í„°(x)ê°€ ë“¤ì–´ì™”ì„ ë•Œ ê³„ì‚° ìˆœì„œë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
        (ìš°ë¦¬ê°€ ë°©ê¸ˆ ë°°ìš´ ìˆ˜ì‹ ê·¸ëŒ€ë¡œì…ë‹ˆë‹¤)
        """
        # 1. Q, K, V ìƒì„±
        Q = self.W_q(x) # (Batch, Seq, d_k)
        K = self.W_k(x) # (Batch, Seq, d_k)
        V = self.W_v(x) # (Batch, Seq, d_v)
        
        # 2. Attention(Q, K, V) ê³„ì‚°
        # (Batch, Seq, d_k) @ (Batch, d_k, Seq) -> (Batch, Seq, Seq)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        attention_weights = F.softmax(scores, dim=-1)
        
        # (Batch, Seq, Seq) @ (Batch, Seq, d_v) -> (Batch, Seq, d_v)
        output = torch.matmul(attention_weights, V)
        
        return output

# --- 2. ë”¥ëŸ¬ë‹ ëª¨ë¸ì—ì„œ 'ì ìš©'í•˜ê¸° ---
# ì´ì œ ì´ ì–´í…ì…˜ ë ˆì´ì–´ë¥¼ 'ë¶€í’ˆ'ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ë” í° ëª¨ë¸ì„ ë§Œë“­ë‹ˆë‹¤.
class MySimpleTransformerBlock(nn.Module):
    def __init__(self, embed_dim, d_k, d_v):
        super().__init__()
        
        # 1. ë°©ê¸ˆ ë§Œë“  ì…€í”„-ì–´í…ì…˜ ë ˆì´ì–´ë¥¼ 'ë¶€í’ˆ'ìœ¼ë¡œ ì¥ì°©!
        self.attention = SelfAttentionHead(embed_dim, d_k, d_v)
        
        # 2. MLP (íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ì–´í…ì…˜ ë’¤ì— MLPë¥¼ ë¶™ì—¬ì¤ë‹ˆë‹¤)
        self.mlp = nn.Sequential(
            nn.Linear(d_v, embed_dim * 2), # d_v -> ë” í¬ê²Œ
            nn.ReLU(),
            nn.Linear(embed_dim * 2, embed_dim) # ë‹¤ì‹œ ì›ë˜ëŒ€ë¡œ
        )
        
        # (ì‹¤ì œ íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” LayerNorm, Skip Connection ë“±ë„ ìˆì§€ë§Œ í•µì‹¬ì€ ì´ ë‘ ê°œ)

    def forward(self, x):
        # 1. ì…€í”„-ì–´í…ì…˜ ë ˆì´ì–´ë¥¼ í†µê³¼ì‹œì¼œ ë¬¸ë§¥ì„ íŒŒì•…
        attn_output = self.attention(x)
        
        # 2. MLP ë ˆì´ì–´ë¥¼ í†µê³¼ì‹œì¼œ ë” ê¹Šì€ ì²˜ë¦¬
        final_output = self.mlp(attn_output)
        
        return final_output

# --- 3. ì‹¤í–‰í•´ë³´ê¸° ---
embed_dim = 4 # (ê°€ì •)
d_k = 4
d_v = 4

# ëª¨ë¸ ìƒì„± (ìš°ë¦¬ê°€ ë§Œë“  ì–´í…ì…˜ ë ˆì´ì–´ê°€ ë‚´ì¥ë¨)
model = MySimpleTransformerBlock(embed_dim, d_k, d_v)

# ê°€ìƒì˜ ì…ë ¥ ë°ì´í„° (ë°°ì¹˜=1, ë‹¨ì–´=3ê°œ, ì°¨ì›=4)
x = torch.rand(1, 3, embed_dim)

# ëª¨ë¸ì— ì ìš©!
output = model(x)

print(f"ì…ë ¥ x shape: {x.shape}")
print(f"ìµœì¢… ì¶œë ¥ output shape: {output.shape}")
```

ì´ `output` í…ì„œ(shape `(1, 3, 4)`)ê°€ ë°”ë¡œ ì…€í”„-ì–´í…ì…˜ì„ í†µê³¼í•œ ìƒˆë¡œìš´ ë‹¨ì–´ ë²¡í„°ì…ë‹ˆë‹¤.

ì…ë ¥ `x`ì™€ ëª¨ì–‘ì€ ê°™ì§€ë§Œ, ì´ì œ `output`ì˜ ê° ë‹¨ì–´ ë²¡í„°(ì˜ˆ: ì²« ë²ˆì§¸ ë‹¨ì–´)ëŠ” 'animal'ì˜ ì˜ë¯¸(V)ë¥¼ 88% ì°¸ê³ í•˜ê³  'street'ì˜ ì˜ë¯¸(V)ë¥¼ 3% ì°¸ê³ í•˜ëŠ” ì‹ìœ¼ë¡œ, ë¬¸ë§¥ ì „ì²´ì˜ ì˜ë¯¸ê°€ í’ë¶€í•˜ê²Œ ë°˜ì˜ëœ ë²¡í„°ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.

ì´ê²ƒì´ **'ë‹¨ì¼ ì–´í…ì…˜ í—¤ë“œ(Single-Head Attention)'**ì˜ ì „ì²´ ê³„ì‚° ê³¼ì •ì…ë‹ˆë‹¤.

ê·¸ëŸ°ë° ì‹¤ì œ íŠ¸ëœìŠ¤í¬ë¨¸ ë…¼ë¬¸(Attention Is All You Need)ì—ì„œëŠ” ì´ ê³¼ì •ì„ í•œ ë²ˆë§Œ í•˜ì§€ ì•Šê³ , ì—¬ëŸ¬ ê°œì˜ 'ì–´í…ì…˜ í—¤ë“œ'ë¥¼ ë™ì‹œì— ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ëŠ” **'ë©€í‹°-í—¤ë“œ ì–´í…ì…˜(Multi-Head Attention)'** ğŸ§ ğŸ§ ğŸ§ ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

ì™œ êµ³ì´ 'ë©€í‹°-í—¤ë“œ'ë¥¼ ì“¸ê¹Œìš”? í•œ ë²ˆë§Œ ê³„ì‚°í•˜ë©´ ì•ˆ ë˜ëŠ” ê±¸ê¹Œìš”?

ì •í™•íˆëŠ”, ë¬¸ì¥ì„ 'ë” ê¹Šì´ ì´í•´í•˜ê¸° ìœ„í•´ì„œ'ì˜ˆìš”. ğŸ’¯

ìš°ë¦¬ê°€ ë°©ê¸ˆ ë§Œë“  'ì‹±ê¸€-í—¤ë“œ' ì–´í…ì…˜ì€ í•œ ë²ˆì— í•œ ê°€ì§€ ê´€ê³„ë§Œ ë³¼ ìˆ˜ ìˆì–´ìš”.

ì˜ˆë¥¼ ë“¤ì–´ "The animal didn't cross the street because **it** was too tired."ë¼ëŠ” ë¬¸ì¥ì—ì„œ,

- ìš°ë¦¬ì˜ 'ì‹±ê¸€-í—¤ë“œ'ëŠ” 'it'ì´ 'animal'ì„ ê°€ë¦¬í‚¨ë‹¤ëŠ” ê²ƒ(ì£¼ì–´ ê´€ê³„)ì„ í•™ìŠµí•  ìˆ˜ ìˆì„ ê±°ì˜ˆìš”.
    

í•˜ì§€ë§Œ 'it'ì´ 'was too tired'ë¼ëŠ” ìƒíƒœ(ì„œìˆ ì–´ ê´€ê³„)ì™€ë„ ì—°ê²°ëœë‹¤ëŠ” ê²ƒì€ ë™ì‹œì— íŒŒì•…í•˜ê¸° ì–´ë ¤ìš¸ ìˆ˜ ìˆì–´ìš”.

**ë©€í‹°-í—¤ë“œ ì–´í…ì…˜ (Multi-Head Attention) ğŸ§ ğŸ§ ğŸ§ **ì€ ì´ ì–´í…ì…˜ ê³„ì‚°ì„ **ì—¬ëŸ¬ ë²ˆ ë³‘ë ¬ë¡œ** ìˆ˜í–‰í•˜ëŠ” ê±°ì˜ˆìš”.

- **í—¤ë“œ 1:** 'it' $\leftrightarrow$ 'animal' (ì£¼ì–´ ê´€ê³„)ë¥¼ ë‹´ë‹¹
    
- **í—¤ë“œ 2:** 'it' $\leftrightarrow$ 'was too tired' (ìƒíƒœ ê´€ê³„)ë¥¼ ë‹´ë‹¹
    
- **í—¤ë“œ 3:** 'it' $\leftrightarrow$ 'because' (ì´ìœ  ê´€ê³„)ë¥¼ ë‹´ë‹¹
    
- ...
    

ì´ë ‡ê²Œ ì—¬ëŸ¬ ê°œì˜ 'ì‹œì 'ìœ¼ë¡œ ë¬¸ì¥ì„ ë™ì‹œì— ë¶„ì„í•œ ë’¤, ê·¸ ê²°ê³¼ë“¤ì„ ë‹¤ì‹œ í•˜ë‚˜ë¡œ í•©ì³ì„œ ë¬¸ë§¥ì„ í›¨ì”¬ í’ë¶€í•˜ê²Œ ì´í•´í•˜ëŠ” ê±°ì£ .

### ë©€í‹°-í—¤ë“œ ì–´í…ì…˜

ìš°ë¦¬ê°€ ë°©ê¸ˆ ë§Œë“  `SelfAttentionHead`(ì‹±ê¸€ í—¤ë“œ)ë¥¼ **ì—¬ëŸ¬ ê°œ ë§Œë“¤ì–´ì„œ ë³‘ë ¬ë¡œ ì‹¤í–‰**í•œ ë‹¤ìŒ, ê·¸ ê²°ê³¼ë“¤ì„ **í•˜ë‚˜ë¡œ ë‹¤ì‹œ í•©ì¹˜ëŠ”** ê±°ì˜ˆìš”.

ì´ì „ êµì•ˆì—ì„œ ë§Œë“  `SelfAttentionHead` í´ë˜ìŠ¤ë¥¼ 'ë¶€í’ˆ'ìœ¼ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ì„œ `MultiHeadAttention` í´ë˜ìŠ¤ë¥¼ ì¡°ë¦½í•´ ë³´ê² ìŠµë‹ˆë‹¤.

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

# --- 1. ë¶€í’ˆ: ì‹±ê¸€-í—¤ë“œ ì–´í…ì…˜ (ì´ì „ êµì•ˆ) ---
# (ì´ í´ë˜ìŠ¤ë¥¼ ê·¸ëŒ€ë¡œ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤)
class SelfAttentionHead(nn.Module):
    def __init__(self, embed_dim, d_k, d_v):
        """
        __init__: Q, K, Vë¥¼ ë§Œë“œëŠ” Linear ì¸µë“¤ì„ ì •ì˜
        """
        super().__init__()
        self.d_k = d_k
        self.W_q = nn.Linear(embed_dim, d_k)
        self.W_k = nn.Linear(embed_dim, d_k)
        self.W_v = nn.Linear(embed_dim, d_v)

    def forward(self, x):
        """
        forward: Attention(Q, K, V) ìˆ˜ì‹ ê³„ì‚°
        """
        Q = self.W_q(x) # (Batch, Seq, d_k)
        K = self.W_k(x) # (Batch, Seq, d_k)
        V = self.W_v(x) # (Batch, Seq, d_v)
        
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        attention_weights = F.softmax(scores, dim=-1)
        
        output = torch.matmul(attention_weights, V)
        return output # (Batch, Seq, d_v)

# --- 2. ì¡°ë¦½: ë©€í‹°-í—¤ë“œ ì–´í…ì…˜ í´ë˜ìŠ¤ ---
class MultiHeadAttention(nn.Module):
    def __init__(self, num_heads, embed_dim):
        """
        __init__: ì—¬ëŸ¬ ê°œì˜ 'ì‹±ê¸€ í—¤ë“œ'ì™€ ìµœì¢… ì¶œë ¥ ì¸µì„ ì •ì˜
        
        num_heads (int): í—¤ë“œì˜ ê°œìˆ˜ (ì˜ˆ: 8)
        embed_dim (int): ì…ë ¥ ì„ë² ë”© ì°¨ì› (ì˜ˆ: 512)
        """
        super().__init__()
        assert embed_dim % num_heads == 0, "ì„ë² ë”© ì°¨ì›ì€ í—¤ë“œ ê°œìˆ˜ë¡œ ë‚˜ëˆ„ì–´ ë–¨ì–´ì ¸ì•¼ í•©ë‹ˆë‹¤."
        
        self.num_heads = num_heads
        self.embed_dim = embed_dim
        
        # ê° í—¤ë“œê°€ ê°€ì§ˆ Q, K, Vì˜ ì°¨ì›
        # ì˜ˆ: 512 / 8 = 64
        self.d_k = embed_dim // num_heads
        self.d_v = embed_dim // num_heads
        
        # 1. 'SelfAttentionHead'ë¥¼ num_heads ê°œìˆ˜ë§Œí¼ ë¦¬ìŠ¤íŠ¸ì— ë‹´ì•„ë‘¡ë‹ˆë‹¤.
        # nn.ModuleListëŠ” PyTorchê°€ ì´ ë¦¬ìŠ¤íŠ¸ ì•ˆì˜ ëª¨ë“ˆë“¤ë„ 
        # í•™ìŠµ ëŒ€ìƒ(íŒŒë¼ë¯¸í„°)ì„ì„ ì•Œê²Œ í•´ì¤ë‹ˆë‹¤.
        self.heads = nn.ModuleList([
            SelfAttentionHead(embed_dim, self.d_k, self.d_v) 
            for _ in range(num_heads)
        ])
        
        # 2. ëª¨ë“  í—¤ë“œì˜ ì¶œë ¥ì„ í•˜ë‚˜ë¡œ í•©ì³ì¤„ Linear ì¸µ
        # (í—¤ë“œ 8ê°œ * d_v 64) = 512 -> 512
        self.fc_out = nn.Linear(num_heads * self.d_v, embed_dim)
        
    def forward(self, x):
        # 1. ëª¨ë“  í—¤ë“œë¥¼ ë³‘ë ¬ë¡œ ì‹¤í–‰
        # (ê° í—¤ë“œì˜ ì¶œë ¥ì€ (Batch, Seq, d_v) í˜•íƒœ)
        head_outputs = [head(x) for head in self.heads]
        
        # 2. ê²°ê³¼ë“¤ì„ ë§ˆì§€ë§‰ ì°¨ì›(dim=-1) ê¸°ì¤€ìœ¼ë¡œ í•˜ë‚˜ë¡œ í•©ì¹©ë‹ˆë‹¤(concatenate).
        # (Batch, Seq, d_v) 8ê°œê°€ -> (Batch, Seq, d_v * 8)
        concat_output = torch.cat(head_outputs, dim=-1)
        
        # 3. ìµœì¢… Linear ì¸µì„ í†µê³¼ì‹œì¼œ ì›ë˜ì˜ embed_dimìœ¼ë¡œ ë³€í™˜
        # (Batch, Seq, embed_dim)
        final_output = self.fc_out(concat_output)
        
        return final_output

# --- 3. ì‹¤í–‰í•´ë³´ê¸° ---
# (ê°€ì •) ë°°ì¹˜=1, ë¬¸ì¥ ê¸¸ì´=3, ì„ë² ë”© ì°¨ì›=8
# (í—¤ë“œ 2ê°œë¡œ ìª¼ê°œê¸° ì¢‹ê²Œ ì‘ì€ ìˆ«ìë¡œ)
batch_size = 1
seq_len = 3
embed_dim = 8
num_heads = 2 # í—¤ë“œ 2ê°œ

# ê°€ìƒì˜ ì…ë ¥ ë°ì´í„°
x = torch.rand(batch_size, seq_len, embed_dim)
print(f"ì…ë ¥ x shape: {x.shape}")

# ë©€í‹°-í—¤ë“œ ì–´í…ì…˜ ëª¨ë¸ ìƒì„±
# í—¤ë“œ 2ê°œ, ì…ë ¥ ì°¨ì› 8
# (ê° í—¤ë“œëŠ” d_k=4, d_v=4 ì°¨ì›ì„ ê°–ê²Œ ë¨)
multi_head_attn = MultiHeadAttention(num_heads=num_heads, embed_dim=embed_dim)

# ëª¨ë¸ì— ì ìš©
output = multi_head_attn(x)

print(f"ìµœì¢… ì¶œë ¥ output shape: {output.shape}")
```

ë³´ì‹œë‹¤ì‹œí”¼, ë©€í‹°-í—¤ë“œ ì–´í…ì…˜ì€ `SelfAttentionHead`ë¼ëŠ” ë¶€í’ˆì„ `num_heads` ê°œìˆ˜ë§Œí¼ ë³‘ë ¬ë¡œ ëŒë¦¬ê³ , `torch.cat`ìœ¼ë¡œ í•©ì¹œ ë’¤, `nn.Linear`ë¡œ ë§ˆë¬´ë¦¬í•˜ëŠ” 'ì¡°ë¦½' ê³¼ì •ì…ë‹ˆë‹¤.

`output`ì˜ ëª¨ì–‘ì´ `x`ì˜ ëª¨ì–‘ê³¼ `(1, 3, 8)`ë¡œ ë™ì¼í•˜ê²Œ ë‚˜ì™”ì£ ? ì´ê²ƒì´ í•µì‹¬ì…ë‹ˆë‹¤! ì´ `MultiHeadAttention` ë ˆì´ì–´ëŠ” **ì…ë ¥ê³¼ ì¶œë ¥ì˜ ëª¨ì–‘ì´ ê°™ê¸° ë•Œë¬¸ì—**, ë§ˆì¹˜ `nn.Linear`ì²˜ëŸ¼ ì–¼ë§ˆë“ ì§€ **ì—¬ëŸ¬ ì¸µìœ¼ë¡œ ê¹Šê²Œ ìŒ“ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.**

---

ì, ì´ì œ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ í•µì‹¬ ì—”ì§„(ì–´í…ì…˜)ì„ 'ë°”ë‹¥ë¶€í„°' ì½”ë“œë¡œ êµ¬í˜„í•´ ë´¤ìŠµë‹ˆë‹¤. ë”¥ëŸ¬ë‹ì—ì„œ ê°€ì¥ ë³µì¡í•œ ë¶€í’ˆ ì¤‘ í•˜ë‚˜ë¥¼ ë§ˆìŠ¤í„°í•˜ì‹  ê±°ì˜ˆìš”!