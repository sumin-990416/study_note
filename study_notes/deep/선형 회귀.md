**1. 개념: 선형 회귀란?**

- 선형 회귀는 데이터( $x$ )와 정답( $y$ ) 사이에 **직선 관계**가 있다고 가정하고, 데이터를 가장 잘 표현하는 **하나의 직선**을 찾는 문제입니다.
    
- ML에서는 이 직선을 $y = Wx + b$ (또는 $H(x) = Wx + b$)라는 **가설(Hypothesis)**로 표현합니다.
    
- **학습의 목표:** 수많은 데이터 $(x, y)$를 보고, 이들을 가장 잘 설명하는 최적의 **가중치(Weight, $W$)**와 **편향(Bias, $b$)**을 찾는 것입니다.
    
    - $W$: 직선의 기울기
        
    - $b$: y절편
![[Pasted image 20251021105525.png]]

**2. 딥러닝 관점에서의 선형 회귀**

- 딥러닝에서 선형 회귀 모델 $H(x) = Wx + b$ 는 **가장 단순한 형태의 인공 신경망**으로 볼 수 있습니다.
    
- 입력( $x$ )을 받아, 가중치( $W$ )를 곱하고, 편향( $b$ )을 더해, 예측( $H(x)$ )을 내보내는 하나의 **'뉴런'** 또는 '레이어' 라고 생각할 수 있습니다.
    

**3. 핵심 구성 요소**

**A. 가설 (Hypothesis) - 모델 정의**

- 우리가 찾으려는 직선의 방정식입니다.
    
- $H(x) = Wx + b$
    
- PyTorch에서는 이 기능을 **`torch.nn.Linear`** 라는 모듈로 미리 만들어 두었습니다.
    

**B. 손실 함수 (Loss Function) - 오차 측정**

- "모델의 예측( $H(x)$ )이 실제 정답( $y$ )과 얼마나 다른가?"를 측정하는 함수입니다.
    
- 선형 회귀에서는 주로 **평균 제곱 오차(Mean Squared Error, MSE)**를 사용합니다.
    
- 수식:
    
    $$\text{Loss (MSE)} = \frac{1}{N} \sum_{i=1}^{N} (H(x_i) - y_i)^2$$
    
    - $N$: 데이터 개수
        
    - $H(x_i)$: $i$번째 데이터 $x_i$에 대한 모델의 **예측값**
        
    - $y_i$: $i$번째 데이터 $x_i$의 실제 정답
        
- PyTorch에서는 **`torch.nn.MSELoss`** 로 미리 만들어 두었습니다.
    

**C. 옵티마이저 (Optimizer) - 파라미터 업데이트**

- 손실(Loss)을 최소화하기 위해 $W$ 와 $b$ 를 "어떻게" 업데이트할지 결정하는 방법입니다.
    
- 가장 기본이 되는 **경사 하강법 (Stochastic Gradient Descent, SGD)**을 사용합니다.
    
- **작동 원리:**
    
    1. 손실 함수를 $W$ 와 $b$ 로 각각 미분하여 **기울기(gradient)**를 구합니다. (이 과정을 `loss.backward()`가 자동으로 해줍니다.)
        
    2. 이 기울기 값의 **반대 방향**으로 $W$ 와 $b$ 를 조금씩 이동시킵니다.
        
    3. 이때 '얼마나' 이동할지 결정하는 값을 **학습률(Learning Rate)**이라고 합니다.
        
- PyTorch에서는 `torch.optim.SGD` 등으로 구현되어 있습니다.
    

**4. PyTorch를 이용한 학습 과정 (The Training Loop)**

딥러닝 모델은 다음과 같은 정형화된 반복(Loop) 과정을 통해 학습됩니다.

1. **모델 정의:** `model = torch.nn.Linear(...)`
    
2. **손실 함수 정의:** `criterion = torch.nn.MSELoss()`
    
3. **옵티마이저 정의:** `optimizer = torch.optim.SGD(model.parameters(), lr=0.01)`
    

---

**[학습 반복 (Epoch)]**

- **(Forward Pass)**: 모델에 입력( $x$ )을 넣어 예측( $H(x)$ )을 계산 $\rightarrow$ 이 예측값과 정답( $y$ )으로 손실(Loss)을 계산.
    
- `optimizer.zero_grad()`: **(중요!)** 이전에 계산된 미분값을 0으로 초기화. (이걸 안 하면 미분값이 계속 누적됩니다.)
    
- **(Backward Pass)**: `loss.backward()` 호출 $\rightarrow$ **(Autograd 작동!)** 손실에 대한 $W$ 와 $b$ 의 미분값(gradient)을 자동으로 계산.
    
- `optimizer.step()`: 계산된 미분값을 바탕으로 $W$ 와 $b$ 를 업데이트 (학습 실행).
    

---

_이 과정을 수백, 수천 번 반복하면 모델의 $W$ 와 $b$ 가 정답에 가까워집니다._


```python
import torch
import torch.nn as nn  # 신경망(Neural Network) 관련 모듈
import torch.optim as optim  # 옵티마이저(최적화) 관련 모듈

# --- 0. 데이터 준비 ---
# x가 1, 2, 3일 때 y가 2, 4, 6인 데이터를 가정 (y = 2x)
# .view(-1, 1)은 데이터를 [1], [2], [3] 형태의 2차원 텐서로 만듭니다.
# 딥러닝 모델은 배치(batch) 입력을 가정하므로, (데이터 개수, 특성 개수) 형태가 필요합니다.
x_train = torch.tensor([[1.0], [2.0], [3.0]])
y_train = torch.tensor([[2.0], [4.0], [6.0]])

# --- 1. 모델(가설) 정의 ---
# H(x) = Wx + b
# 입력 차원(x)이 1개, 출력 차원(y)이 1개인 선형 레이어(Linear Layer)를 정의
# model = nn.Linear(input_dim, output_dim)
model = nn.Linear(1, 1)

# --- 2. 손실 함수(Loss) 및 옵티마이저(Optimizer) 정의 ---
# 손실 함수로 평균 제곱 오차(MSE)를 사용
criterion = nn.MSELoss()

# 옵티마이저로 확률적 경사 하강법(SGD)을 사용
# model.parameters()는 모델의 학습 대상(W, b)을 옵티마이저에게 알려줍니다.
# lr (learning_rate, 학습률): W와 b를 한 번에 얼마나 업데이트할지 정하는 값
optimizer = optim.SGD(model.parameters(), lr=0.01)

# --- 3. 학습(Training) 실행 ---
# epochs: 전체 데이터를 몇 번 반복해서 학습할지 횟수를 정합니다.
epochs = 1000  

for epoch in range(epochs + 1):
    
    # 1. (Forward Pass) 모델 예측
    # x_train을 모델에 입력하여 예측값 H(x)를 계산합니다.
    prediction = model(x_train)
    
    # 2. (Forward Pass) 손실(Loss) 계산
    # 예측값(prediction)과 실제 정답(y_train)을 비교하여 손실(오차)을 계산합니다.
    loss = criterion(prediction, y_train)
    
    # 3. (Backward Pass 준비) 미분값 초기화
    # loss.backward()를 호출하기 전에, 
    # 이전 epoch에서 계산된 미분값(gradient)이 누적되지 않도록 0으로 초기화합니다.
    optimizer.zero_grad()
    
    # 4. (Backward Pass) 자동 미분 실행
    # (Autograd 작동!) 손실(loss)을 기준으로 W와 b에 대한 미분값을 자동으로 계산합니다.
    loss.backward()
    
    # 5. (Update) 파라미터 업데이트
    # 계산된 미분값을 바탕으로 옵티마이저가 W와 b를 업데이트(학습)합니다.
    optimizer.step()
    
    # 100번에 한 번씩 학습 과정 출력
    if epoch % 100 == 0:
        print(f'Epoch {epoch:4d}/{epochs} | Loss: {loss.item():.6f}')

# --- 4. 학습 결과 확인 ---
# 학습이 완료된 후의 W와 b는 model.parameters()에서 확인할 수 있습니다.
# (우리의 목표는 W=2, b=0 에 가까워지는 것입니다)
print("\n--- 학습 완료 후 ---")
# list(model.parameters())[0]는 W, [1]은 b 입니다.
print(f"최종 W (기울기): {list(model.parameters())[0].item():.4f}")
print(f"최종 b (절편): {list(model.parameters())[1].item():.4f}")

# 새로운 데이터 [4.0]으로 예측해보기
new_input = torch.tensor([[4.0]])
predicted_value = model(new_input)
print(f"x=4.0 일 때 예측값 y: {predicted_value.item():.4f} (정답 8.0 근처)")
```

```python
#교안처럼 구조 작성
import torch import torch.nn as nn 
# --- 교안 3-2 (선형 회귀)의 클래스 버전 --- 
# H(x) = Wx + b 
class LinearRegressionModel(nn.Module): 
	def __init__(self, input_dim, output_dim): 
	""" 
	모델의 구성요소(레이어)를 정의합니다. 
	super().__init__() 는 항상 처음에 호출해야 합니다. 
	""" 
	super().__init__() 
	# Wx + b 를 계산하는 단일 선형 레이어 1개만 필요합니다. 
	self.linear = nn.Linear(input_dim, output_dim) 
	
	def forward(self, x): 
	""" 
	데이터(x)가 입력으로 들어왔을 때, __init__에서 
	정의한 레이어를 통과하는 순서를 정의합니다. 
	""" 
	# 데이터(x)를 선형 레이어에 통과시킨 결과를 반환합니다. 
	return self.linear(x) 
	
# [모델 선언 및 더미 데이터 테스트] 
# 입력 특성 1개(예: 공부 시간), 출력 1개(예: 성적) 
model_linear = LinearRegressionModel(input_dim=1, output_dim=1) 
# 10명의 학생 데이터, 1개의 특성(공부 시간) 
X_linear = torch.ones(size=(10, 1)) 
y_pred_linear = model_linear(X_linear) 
# print(f"선형 회귀 모델 출력 크기: {y_pred_linear.size()}") 
# torch.Size([10, 1])
```
이 코드가 바로 딥러닝 학습의 가장 기본적인 '반복문(Training Loop)' 구조입니다. 
**거의 모든 딥러닝 코드가 이 5단계(예측 → 손실계산 → 초기화 → 역전파 → 업데이트)를 따릅니다.**